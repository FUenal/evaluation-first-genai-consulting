
---

# `01_services/services_page.md`

**FINAL – Canonical, Evaluation-First Version**

---

# Services

## Evaluation-First Knowledge System Assessment & Prototype

---

## What this service is

I help organizations design **evaluation-first AI-assisted knowledge systems** that can be meaningfully evaluated, governed, and trusted **before deployment**.

The focus is not on building chatbots or pilots, but on defining **acceptance criteria, failure modes, and decision risk** before any system is operationalized.

The outcome is a **clear, evidence-based decision** on whether such a system should exist in your context — and under what constraints.

---

## The problem this service addresses

Most GenAI initiatives fail not because models are weak, but because **success and failure are never defined in a way that reflects real organizational risk**.

As a result:

* systems perform well in demos but fail in real use
* unacceptable errors are discovered too late
* refusal and uncertainty are not designed
* trust erodes quickly

This service is built on the opposite principle:

> **If a system cannot be meaningfully evaluated in its domain context, it should not be deployed.**

---

## Who this service is for

This service is suited for organizations where:

* decisions depend on complex internal documents
* incorrect or misleading answers have legal, financial, or reputational consequences
* explainability, auditability, and governance matter
* data usage is constrained by regulation or policy

Typical contexts include regulated organizations, public institutions, NGOs, and research-driven environments.

---

## Who this service is not for

This service is **not** intended for organizations seeking:

* a ready-made SaaS product
* customer support or sales chatbots
* rapid experimentation or demo-driven pilots
* real-time or mission-critical systems
* ongoing system operation or maintenance

If speed matters more than correctness, this engagement is not a good fit.

---

## Scope & approach (evaluation-first)

The engagement is structured as a **fixed-scope, time-boxed assessment** (typically **6–8 weeks**) and follows a strict evaluation-first sequence.

---

### 1. Use-case & risk framing

We clarify:

* which decisions the system would support
* who remains accountable for outcomes
* what constitutes unacceptable vs tolerable failure
* where the system must refuse to answer

**Deliverable**
Problem & risk framing document

---

### 2. Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* audit and governance requirements
* data gaps and no-go signals

A recommendation **not to proceed** is considered a valid outcome.

**Deliverable**
Data readiness & governance memo

---

### 3. Joint evaluation & system design (with SMEs)

This is the core of the engagement.

In structured working sessions, **Subject Matter Experts co-define**:

* domain-specific acceptance criteria
* failure modes and refusal rules
* evaluation datasets reflecting real decision risk

Evaluation design precedes system optimization.

**Deliverables**

* Evaluation design document
* SME-validated evaluation dataset

---

### 4. Evaluation-guided prototype

A limited-scope prototype is implemented as an **instrumented test harness**, not a production system.

It is used to:

* run evaluations against agreed criteria
* inspect failure modes and trade-offs
* identify irreducible risks

**Deliverables**

* Instrumented prototype
* Evaluation report with failure analysis

---

### 5. Decision memo

The engagement concludes with a structured recommendation to:

> **Proceed · Iterate · Stop**

**Deliverable**
Decision memo suitable for management and governance review

---

## Engagement success criteria

This engagement is successful if, at the end of the assessment, the organization has:

* a documented evaluation framework grounded in domain risk
* a prototype tested against SME-defined acceptance criteria
* a clear, defensible decision to proceed, iterate, or stop

No production deployment is required for success.

---

## Evaluation infrastructure & continuity

An evaluation interface based on **Evaluizer** is used during the engagement and handed over so internal teams can continue evaluation independently.

This enables:

* transparent inspection of system behavior
* repeatable evaluation over time
* regression detection as data or prompts change

No vendor lock-in is introduced.

---

## Engagement model

* Fixed scope
* Fixed duration (6–8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

**This engagement is intentionally conservative and is not designed for rapid experimentation or production deployment.**

---

## Time commitment

* SME involvement is typically limited to **2–4 hours total**, primarily through a structured evaluation workshop
* One decision owner is required for scoping and sign-off

---

## Independence & IP

All work is conducted independently and uses only client-provided or publicly available materials.
No confidential or proprietary information of third parties is reused or derived.

---

## If the recommendation is to stop

If the recommendation is not to proceed, **all evaluation artifacts, decision frameworks, and findings remain usable** for future initiatives, audits, or internal reviews.

Stopping is treated as a valid and valuable outcome.

---

## Next step

If you are considering an AI-assisted knowledge system and want to determine **whether it can be trusted in your domain**, an initial discussion can clarify whether this engagement is appropriate.

---

