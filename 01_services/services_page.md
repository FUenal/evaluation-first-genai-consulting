

---

# Services

## Independent AI Risk & Readiness Audits

---

## What I do

I provide **independent risk and readiness assessments** for AI-assisted knowledge systems.

My role is not to build or optimize AI systems.
My role is to **assess whether they are safe, defensible, and appropriate to deploy** in a given organizational and regulatory context.

I operate as an **external assurance specialist** — comparable to a technical auditor — helping organizations make evidence-based decisions about AI use **before** reputational, legal, or operational harm occurs.

---

## Who this is for

This service is designed for organizations that:

* operate in **regulated or high-trust environments**
* are accountable to regulators, boards, or the public
* feel pressure to “do something with AI” but are unsure what is safe
* want an **independent second opinion**, not vendor optimism

Typical clients include:

* NGOs and foundations
* public sector and cantonal organizations
* regulated SMEs
* compliance- and security-conscious teams

---

## What problem this addresses

Most AI initiatives fail **after deployment**, not during prototyping.

The root cause is rarely model quality.
It is that **“acceptable behavior” is never defined in a way that reflects real organizational risk**.

As a result:

* failure modes are discovered too late
* refusal and uncertainty are not designed
* accountability is unclear
* governance becomes reactive

My work addresses this gap by placing **risk, failure, and decision defensibility** at the center of AI system assessment.

---

## My role (and what I explicitly do not do)

I act as an **independent assessor**.

I:

* evaluate system behavior against domain-specific risk criteria
* classify failure modes and refusal gaps
* assess regulatory and governance exposure
* produce clear, defensible recommendations

I explicitly do **not**:

* implement or deploy systems
* tune models or prompts
* own production outcomes
* provide legal certification or sign-off

This independence is deliberate and non-negotiable.

---

## Core services (Phase-1)

All services are **time-boxed, audit-shaped, and evidence-based**.

---

### Risk & Safety Workshop

**Primary entry point**

A structured workshop with Subject Matter Experts and decision owners to define:

* what “safe” and “acceptable” mean in your domain
* which failure modes are unacceptable
* when an AI system must refuse to answer

**Outcome**

* documented risk map
* explicit acceptance and refusal criteria
* clarity on whether further assessment is justified

---

### Vendor Safety Audit

An independent assessment of AI vendors or AI-enabled SaaS products.

Focus areas include:

* data usage and training rights
* data residency and control
* architectural and contractual red flags

**Outcome**

* short audit memo
* clear recommendation: *Proceed · Conditional · Do not proceed*

---

### Adversarial Behavior Assessment

A black-box stress test of an existing AI prototype or system.

The goal is to surface:

* hallucination and unsupported inference
* overconfidence and omission
* refusal failures
* misalignment with domain norms

This is a **behavioral risk assessment**, not penetration testing.

**Outcome**

* classified failure report
* severity assessment
* go / no-go signal

---

### EU AI Act Gap Analysis (Scoped)

A non-certifying readiness assessment focused on **exposure**, not compliance guarantees.

This service helps organizations understand:

* whether their use case is likely to be classified as high-risk
* where governance, documentation, or transparency gaps exist

**Outcome**

* gap matrix
* risk classification
* decision-oriented next steps

---

## How engagements work

All engagements follow the same structure:

1. **Risk framing**
2. **SME-defined acceptance criteria**
3. **Failure classification**
4. **Evidence-based assessment**
5. **Formal recommendation**

Possible outcomes are:

* **Proceed**
* **Iterate**
* **Stop**

A recommendation to *stop* is considered a **successful outcome** if risk is unacceptable.

---

## What you receive

Depending on the service, deliverables may include:

* audit memos
* risk maps and failure taxonomies
* evaluation evidence
* defensible recommendations for decision owners

If an engagement ends with “Stop,” you still receive:

* clear documentation of why
* governance-ready artifacts
* material that can be used internally or with other vendors

---

## Availability & engagement constraints

To preserve independence and quality:

* I operate as a **solo assessor**
* engagements are **fixed-scope and time-boxed**
* synchronous work is limited and scheduled
* data access delays do not extend engagement duration

This ensures focus, predictability, and audit-grade rigor.

---

## A note on conservatism

My approach is intentionally conservative.

If risk cannot be bounded, or acceptable behavior cannot be defined, the correct recommendation is **not to deploy**.

This is not resistance to innovation.
It is respect for accountability.

---

## Next step

Most engagements start with a **Risk & Safety Workshop**.

If you are unsure whether your AI initiative is safe, appropriate, or worth pursuing, this is the right place to begin.

---

