
---

# ‚ÄúDerived from /01_services/services_page.md (canonical)‚Äù

# Services (Evaluation-First Version, Publication-Ready)

## Evaluation-First Knowledge System Assessment & Prototype

### What this service is

I help organizations **design AI-assisted knowledge systems that can be meaningfully evaluated, governed, and trusted before they are deployed**.

Most GenAI initiatives fail not because the models are weak, but because *success is never defined in a way that reflects real organizational risk*. This service addresses that gap by placing **evaluation and decision quality** at the center of system design.

The outcome is not a demo chatbot, but a **clear, evidence-based decision** on whether such a system should exist in your context ‚Äî and under what constraints.

---

### Why this approach

Industry experience shows that the majority of GenAI pilots fail in production due to:

* undefined or implicit success criteria
* lack of domain-specific evaluation data
* no error taxonomy or acceptance thresholds
* inability to reason about failure modes
* missing human-in-the-loop design

This service is built around the opposite principle:

> **If a system cannot be evaluated in a way that reflects domain risk, it should not be deployed.**

---

### Who this service is for

This service is suited for organizations where:

* decisions depend on complex internal documents or policies
* incorrect answers have legal, financial, or reputational consequences
* AI systems must be auditable and explainable
* data use is constrained by regulation or governance
* there is pressure to ‚Äúuse GenAI‚Äù, but uncertainty about how to do so responsibly

Typical contexts include public institutions, regulated organizations, NGOs, and research-driven environments.

---

### Who this service is *not* for

This service is not intended for organizations seeking:

* a ready-made SaaS chatbot
* customer support or sales automation
* real-time or mission-critical AI systems
* rapid prototyping without evaluation rigor
* ongoing system operations or maintenance

If speed matters more than correctness, this engagement is not a good fit.

---

## Scope & approach (Evaluation-first)

The engagement is structured as a **time-boxed assessment and prototype**, typically over **6‚Äì8 weeks**, and is organized around evaluation rather than implementation speed.

---

### 1. Use-case & risk framing

We begin by clarifying:

* which decisions the system would support
* what constitutes a *wrong* or *unacceptable* answer
* which failure modes are tolerable vs unacceptable
* where the system must refuse to answer
* who remains accountable for outcomes

This step anchors the entire evaluation design.

**Deliverable:**
Problem & risk framing document

---

### 2. Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* data quality and coverage gaps
* governance and audit requirements

This step often surfaces reasons *not* to proceed ‚Äî which is considered a valid outcome.

**Deliverable:**
Data readiness & governance memo (including go / no-go signals)

---

### 3. Joint evaluation & system design (with SMEs)

This is the core differentiator of the engagement.

In structured working sessions with Subject Matter Experts and process owners, we jointly design:

* task-specific evaluation datasets
* domain-relevant success and failure criteria
* error taxonomies (hallucination, omission, ambiguity, outdated information, etc.)
* acceptance thresholds aligned to organizational risk
* human-in-the-loop and override rules

Evaluation design precedes system optimization.

**Deliverables:**

* Evaluation design document
* Initial SME-validated evaluation set

---

### 4. Evaluation-guided prototype

A limited-scope prototype is implemented **as a test harness**, not as a production system.

The prototype is instrumented to:

* run batch evaluations
* inspect retrieval and answer quality
* analyze failure modes
* support regression testing as data or prompts change

The goal is to make failures visible, inspectable, and discussable.

**Deliverables:**

* Instrumented prototype
* Evaluation report with failure analysis and trade-offs

---

### 5. Decision memo

The engagement concludes with a structured decision memo covering:

* whether the system meets acceptance criteria
* what would be required to reach them
* which risks remain irreducible
* whether to proceed, iterate, or stop

**Deliverable:**
Decision memo suitable for internal governance and management review

---

## Evaluation infrastructure & continuity

As part of the engagement, I provide an **evaluation interface and workflow** based on *Evaluizer*, an open-source evaluation framework designed for systematic LLM assessment and optimization.

This allows organizations to:

* visualize model outputs against evaluation datasets
* manually annotate and review results with SMEs
* run automated evaluations using deterministic functions and LLM judges
* track performance over time as prompts, data, or models change

The goal is to **reduce fear around long-term maintenance** and enable internal teams to continue evaluation after the engagement ends.

Evaluizer is adapted and configured for the specific use case and handed over as part of the delivery.

---

## Engagement model

* Fixed scope
* Fixed duration (6‚Äì8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

Engagements are conducted independently using client-provided data and environments, with strict separation from any employer-related activities.

---

## Independence & IP

All engagements use only client-provided or publicly available materials.
I ensure strict separation from any employer-related activities and do not reuse or derive from confidential or proprietary information of third parties.

---

## Next step

If you are considering an AI-assisted knowledge system and want to determine **whether it can be trusted in your domain**, an initial discussion can clarify whether this engagement is appropriate.

---

# Consultant‚Äôs assessment of this positioning

This rewrite does three important things:

1. **Evaluation is no longer a feature ‚Äî it is the product**
2. You sell *decision safety*, not model performance
3. Evaluizer is positioned as **risk-reducing infrastructure**, not a shiny tool

This is exactly how you avoid:

* academic overtones
* tool-centric selling
* being mistaken for an ‚ÄúAI dev shop‚Äù

---

# What we do next (as requested)

Now that the Services page is coherent, the next two artifacts should be:

### Next 1Ô∏è‚É£ SME Workshop Agenda (sales + delivery artifact)

A concrete, 90‚Äì120 minute agenda you can:

* attach to proposals
* walk through in intro calls
* use to show seriousness without code

### Next 2Ô∏è‚É£ ‚ÄúWhy 95% of GenAI Systems Fail‚Äù One-Pager

A short, non-technical explainer you can:

* send before first calls
* use internally with skeptical stakeholders
* anchor your evaluation-first narrative

Both are **high-leverage** and low effort.

---

**My recommendation:**
üëâ Next, we design the **SME Evaluation Workshop agenda**, because it makes the service tangible and dramatically increases conversion quality.

If you agree, say:

> ‚ÄúLet‚Äôs design the SME workshop agenda.‚Äù

We‚Äôll do it at the same level of rigor as everything else.

---

# Services (Evaluation-First Version, Publication-Ready)

## Evaluation-First Knowledge System Assessment & Prototype

### What this service is

I help organizations **design AI-assisted knowledge systems that can be meaningfully evaluated, governed, and trusted before they are deployed**.

Most GenAI initiatives fail not because the models are weak, but because *success is never defined in a way that reflects real organizational risk*. This service addresses that gap by placing **evaluation and decision quality** at the center of system design.

The outcome is not a demo chatbot, but a **clear, evidence-based decision** on whether such a system should exist in your context ‚Äî and under what constraints.

---

### Why this approach

Industry experience shows that the majority of GenAI pilots fail in production due to:

* undefined or implicit success criteria
* lack of domain-specific evaluation data
* no error taxonomy or acceptance thresholds
* inability to reason about failure modes
* missing human-in-the-loop design

This service is built around the opposite principle:

> **If a system cannot be evaluated in a way that reflects domain risk, it should not be deployed.**

---

### Who this service is for

This service is suited for organizations where:

* decisions depend on complex internal documents or policies
* incorrect answers have legal, financial, or reputational consequences
* AI systems must be auditable and explainable
* data use is constrained by regulation or governance
* there is pressure to ‚Äúuse GenAI‚Äù, but uncertainty about how to do so responsibly

Typical contexts include public institutions, regulated organizations, NGOs, and research-driven environments.

---

### Who this service is *not* for

This service is not intended for organizations seeking:

* a ready-made SaaS chatbot
* customer support or sales automation
* real-time or mission-critical AI systems
* rapid prototyping without evaluation rigor
* ongoing system operations or maintenance

If speed matters more than correctness, this engagement is not a good fit.

---

## Scope & approach (Evaluation-first)

The engagement is structured as a **time-boxed assessment and prototype**, typically over **6‚Äì8 weeks**, and is organized around evaluation rather than implementation speed.

---

### 1. Use-case & risk framing

We begin by clarifying:

* which decisions the system would support
* what constitutes a *wrong* or *unacceptable* answer
* which failure modes are tolerable vs unacceptable
* where the system must refuse to answer
* who remains accountable for outcomes

This step anchors the entire evaluation design.

**Deliverable:**
Problem & risk framing document

---

### 2. Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* data quality and coverage gaps
* governance and audit requirements

This step often surfaces reasons *not* to proceed ‚Äî which is considered a valid outcome.

**Deliverable:**
Data readiness & governance memo (including go / no-go signals)

---

### 3. Joint evaluation & system design (with SMEs)

This is the core differentiator of the engagement.

In structured working sessions with Subject Matter Experts and process owners, we jointly design:

* task-specific evaluation datasets
* domain-relevant success and failure criteria
* error taxonomies (hallucination, omission, ambiguity, outdated information, etc.)
* acceptance thresholds aligned to organizational risk
* human-in-the-loop and override rules

Evaluation design precedes system optimization.

**Deliverables:**

* Evaluation design document
* Initial SME-validated evaluation set

---

### 4. Evaluation-guided prototype

A limited-scope prototype is implemented **as a test harness**, not as a production system.

The prototype is instrumented to:

* run batch evaluations
* inspect retrieval and answer quality
* analyze failure modes
* support regression testing as data or prompts change

The goal is to make failures visible, inspectable, and discussable.

**Deliverables:**

* Instrumented prototype
* Evaluation report with failure analysis and trade-offs

---

### 5. Decision memo

The engagement concludes with a structured decision memo covering:

* whether the system meets acceptance criteria
* what would be required to reach them
* which risks remain irreducible
* whether to proceed, iterate, or stop

**Deliverable:**
Decision memo suitable for internal governance and management review

---

## Evaluation infrastructure & continuity

As part of the engagement, I provide an **evaluation interface and workflow** based on *Evaluizer*, an open-source evaluation framework designed for systematic LLM assessment and optimization.

This allows organizations to:

* visualize model outputs against evaluation datasets
* manually annotate and review results with SMEs
* run automated evaluations using deterministic functions and LLM judges
* track performance over time as prompts, data, or models change

The goal is to **reduce fear around long-term maintenance** and enable internal teams to continue evaluation after the engagement ends.

Evaluizer is adapted and configured for the specific use case and handed over as part of the delivery.

---

## Engagement model

* Fixed scope
* Fixed duration (6‚Äì8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

Engagements are conducted independently using client-provided data and environments, with strict separation from any employer-related activities.

---

## Independence & IP

All engagements use only client-provided or publicly available materials.
I ensure strict separation from any employer-related activities and do not reuse or derive from confidential or proprietary information of third parties.

---

## Next step

If you are considering an AI-assisted knowledge system and want to determine **whether it can be trusted in your domain**, an initial discussion can clarify whether this engagement is appropriate.

---

# Consultant‚Äôs assessment of this positioning

This rewrite does three important things:

1. **Evaluation is no longer a feature ‚Äî it is the product**
2. You sell *decision safety*, not model performance
3. Evaluizer is positioned as **risk-reducing infrastructure**, not a shiny tool

This is exactly how you avoid:

* academic overtones
* tool-centric selling
* being mistaken for an ‚ÄúAI dev shop‚Äù

---

# What we do next (as requested)

Now that the Services page is coherent, the next two artifacts should be:

### Next 1Ô∏è‚É£ SME Workshop Agenda (sales + delivery artifact)

A concrete, 90‚Äì120 minute agenda you can:

* attach to proposals
* walk through in intro calls
* use to show seriousness without code

### Next 2Ô∏è‚É£ ‚ÄúWhy 95% of GenAI Systems Fail‚Äù One-Pager

A short, non-technical explainer you can:

* send before first calls
* use internally with skeptical stakeholders
* anchor your evaluation-first narrative

Both are **high-leverage** and low effort.

---

**My recommendation:**
üëâ Next, we design the **SME Evaluation Workshop agenda**, because it makes the service tangible and dramatically increases conversion quality.

If you agree, say:

> ‚ÄúLet‚Äôs design the SME workshop agenda.‚Äù

We‚Äôll do it at the same level of rigor as everything else.

---

# 1Ô∏è‚É£ SME Evaluation Workshop Agenda

**‚ÄúFrom domain knowledge to evaluable system design‚Äù**

### Purpose (how you explain it to clients)

> This workshop translates domain expertise into concrete evaluation criteria, so that an AI-assisted knowledge system can be assessed meaningfully before deployment.

You are not ‚Äúbrainstorming prompts‚Äù.
You are **formalizing judgment**.

---

## Workshop at a glance

* **Duration:** 90‚Äì120 minutes
* **Participants:**

  * 2‚Äì4 Subject Matter Experts
  * 1 process owner / decision-maker
  * (Optional) compliance / risk representative
* **Format:** Structured, facilitated session
* **Output:** Evaluation design inputs, not technical implementation

---

## Pre-work (asynchronous, very light)

You request **before the workshop**:

* 5‚Äì10 representative documents
* 5‚Äì10 example questions people *already ask*
* A short description of decisions these answers inform

This filters unserious engagements early.

---

## Workshop Agenda (Detailed)

---

### 0. Context setting (10 minutes)

**Goal:** Align expectations and psychological safety

You explain:

* This is *not* about building a chatbot
* Wrong answers matter more than impressive answers
* Refusal to answer is a valid outcome
* Evaluation comes before optimization

Key sentence you say out loud:

> ‚ÄúIf we can‚Äôt define what ‚Äògood‚Äô looks like, we shouldn‚Äôt automate it.‚Äù

---

### 1. Decision & risk mapping (20‚Äì25 minutes)

**Goal:** Anchor evaluation in real consequences

You facilitate answers to:

* What decisions would this system support?
* Who relies on these answers?
* What happens if the answer is wrong?
* What happens if the answer is incomplete?
* What happens if the answer sounds confident but is outdated?

You explicitly surface:

* *high-risk vs low-risk questions*
* *where human judgment must remain*

**Output:**

* Decision categories
* Risk tiers (e.g. unacceptable / tolerable / informational)

---

### 2. Failure mode identification (20‚Äì25 minutes)

**Goal:** Make errors explicit and discussable

You guide SMEs through a **failure taxonomy**, such as:

* hallucination (fabricated facts)
* omission (missing critical constraints)
* ambiguity (multiple interpretations)
* outdated information
* misapplied policy
* overconfident tone
* inappropriate refusal

You ask:

* Which of these are unacceptable?
* Which are tolerable?
* Which should trigger refusal?

**Output:**

* Domain-specific error taxonomy
* Initial refusal rules

This is where trust is built.

---

### 3. Defining ‚Äúacceptable answers‚Äù (20‚Äì25 minutes)

**Goal:** Move beyond vague notions of correctness

You work with SMEs to define:

* what *must* be present in a good answer
* what *must never* be present
* when uncertainty should be expressed
* when sources must be cited or constrained

You avoid accuracy percentages.
You talk about **answer properties**.

**Output:**

* Acceptance criteria
* Qualitative scoring dimensions

---

### 4. Evaluation dataset design (20 minutes)

**Goal:** Turn expertise into test cases

Together, you define:

* representative questions per risk tier
* edge cases and ‚Äútrap questions‚Äù
* examples that should trigger refusal
* examples where partial answers are acceptable

You explicitly state:

> ‚ÄúThis is not training data. This is test data.‚Äù

**Output:**

* Initial evaluation question set
* SME-validated expected behaviors

---

### 5. Handover & next steps (10 minutes)

**Goal:** Close cleanly and confidently

You explain:

* how these outputs feed into the prototype
* how Evaluizer will be used to run and inspect evaluations
* what will be tested and what will not
* how results will be reviewed together

You do *not* promise performance.

---

## Workshop Deliverables (what you list in proposals)

* Decision & risk map
* Domain-specific error taxonomy
* Acceptance & refusal criteria
* SME-authored evaluation question set

This alone is worth money.

---

## Why this workshop is a sales accelerator

From a consultant‚Äôs perspective:

* it filters unserious clients
* it demonstrates rigor immediately
* it reframes GenAI as a governance problem
* it positions you as a facilitator of judgment, not a tool builder

---
# Evaluation-First GenAI Consulting ‚Äî Knowledge Base

This repository contains the internal knowledge base, methodologies, and client-facing artifacts for an **evaluation-first consulting practice** focused on AI-assisted knowledge systems.

The core principle is simple:

> If an AI system cannot be meaningfully evaluated in its domain context, it should not be deployed.

The materials in this repository support structured engagements that help organizations decide **whether**, **how**, and **under which constraints** GenAI systems should be used.

---

## What this repository is

- A **structured consulting playbook**
- A collection of **reusable, rigorously defined artifacts**
- A separation of:
  - strategy
  - methodology
  - sales enablement
  - delivery
  - governance

It is designed to support:
- regulated and public-sector environments
- evaluation-first GenAI system design
- SME-driven acceptance criteria
- transparent risk and failure analysis

---

## What this repository is not

- A software product
- A production system
- A marketing website
- A collection of ad-hoc notes

All materials are designed to be **inspectable, explainable, and reusable**.

---

## Repository structure

```text
ai-eval-consulting/
‚îÇ
‚îú‚îÄ‚îÄ 00_strategy/
‚îÇ   ‚îú‚îÄ‚îÄ positioning.md
‚îÇ   ‚îú‚îÄ‚îÄ product_overview.md
‚îÇ   ‚îî‚îÄ‚îÄ differentiation_evaluation_first.md
‚îÇ
‚îú‚îÄ‚îÄ 01_services/
‚îÇ   ‚îú‚îÄ‚îÄ services_page.md
‚îÇ   ‚îú‚îÄ‚îÄ engagement_model.md
‚îÇ   ‚îî‚îÄ‚îÄ scope_out_of_scope.md
‚îÇ
‚îú‚îÄ‚îÄ 02_methodology/
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_first_principles.md
‚îÇ   ‚îú‚îÄ‚îÄ sme_workshop_agenda.md
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_design_framework.md
‚îÇ   ‚îî‚îÄ‚îÄ failure_taxonomy_reference.md
‚îÇ
‚îú‚îÄ‚îÄ 03_sales_enablement/
‚îÇ   ‚îú‚îÄ‚îÄ intro_call_script_15min.md
‚îÇ   ‚îú‚îÄ‚îÄ proposal_template_1pager.md
‚îÇ   ‚îú‚îÄ‚îÄ why_95_percent_fail_onepager.md
‚îÇ   ‚îî‚îÄ‚îÄ faq_objections.md
‚îÇ
‚îú‚îÄ‚îÄ 04_delivery/
‚îÇ   ‚îú‚îÄ‚îÄ delivery_timeline_6_8_weeks.md
‚îÇ   ‚îú‚îÄ‚îÄ client_intake_checklist.md
‚îÇ   ‚îî‚îÄ‚îÄ decision_memo_template.md
‚îÇ
‚îú‚îÄ‚îÄ 05_tools_evaluizer/
‚îÇ   ‚îú‚îÄ‚îÄ evaluizer_overview.md
‚îÇ   ‚îú‚îÄ‚îÄ evaluizer_demo_flow.md
‚îÇ   ‚îú‚îÄ‚îÄ evaluizer_client_handover.md
‚îÇ   ‚îî‚îÄ‚îÄ evaluizer_customization_notes.md
‚îÇ
‚îú‚îÄ‚îÄ 06_governance_ip_coi/
‚îÇ   ‚îú‚îÄ‚îÄ conflict_of_interest_policy_internal.md
‚îÇ   ‚îú‚îÄ‚îÄ ip_boundary_statement_external.md
‚îÇ   ‚îî‚îÄ‚îÄ engagement_documentation_template.md
‚îÇ
‚îú‚îÄ‚îÄ 07_case_materials/
‚îÇ   ‚îú‚îÄ‚îÄ ngo_case_study_draft.md
‚îÇ   ‚îú‚îÄ‚îÄ anonymized_examples.md
‚îÇ   ‚îî‚îÄ‚îÄ future_case_studies/
‚îÇ
‚îú‚îÄ‚îÄ 08_website/
‚îÇ   ‚îú‚îÄ‚îÄ homepage_copy.md
‚îÇ   ‚îú‚îÄ‚îÄ services_page_copy.md
‚îÇ   ‚îú‚îÄ‚îÄ about_page_copy.md
‚îÇ   ‚îî‚îÄ‚îÄ contact_page_copy.md
‚îÇ
‚îî‚îÄ‚îÄ 09_admin_legal/
    ‚îú‚îÄ‚îÄ contract_outline.md
    ‚îú‚îÄ‚îÄ pricing_notes.md
    ‚îî‚îÄ‚îÄ business_setup_notes.md

---

# Evaluizer Demo Flow

**Making evaluation concrete, inspectable, and maintainable**

## Purpose of the demo (internal)

* Reduce fear around ‚Äúlong, unstructured evaluation‚Äù
* Show that evaluation is **operational**, not theoretical
* Demonstrate continuity **after** the engagement ends
* Reinforce that this is *not* model magic, but disciplined process

This demo is **not technical**.
It is about *how decisions are supported*.

---

## When to use this demo

* After a positive intro call
* After the SME Evaluation Workshop
* Before finalizing scope or contract
* With stakeholders worried about:

  * maintenance
  * regression
  * ‚Äúwhat happens when you leave‚Äù

---

## Demo duration

* **10‚Äì15 minutes**
* Live screen share *or* narrated screenshots
* No deep dives into code or models

---

## Demo structure (step-by-step)

---

## Step 1 ‚Äî Start with the evaluation dataset (2‚Äì3 minutes)

**What you show**

The Evaluizer interface with:

* a list of evaluation questions
* grouped by risk tier (e.g. high-risk, informational, refusal cases)

**What you say**

> Before we look at any answers, we start with the evaluation set.
>
> These are questions defined or validated by your SMEs ‚Äî not generic prompts.
>
> This dataset represents *what matters* in your domain, including edge cases and questions the system should refuse to answer.

**Key message**

* Evaluation starts with **domain judgment**, not model output.

---

## Step 2 ‚Äî Inspect answers side-by-side (3 minutes)

**What you show**

* One evaluation question
* Multiple model outputs (or multiple prompt versions)
* Source references (if available)

**What you say**

> Here we can inspect answers side-by-side against the same question.
>
> The goal is not to ask ‚Äúwhich answer sounds best‚Äù, but:
>
> * Is it complete?
> * Is it grounded in the right documents?
> * Does it introduce information that should not be there?
> * Should it have refused instead?

**Key message**

* Answers are **inspectable**, not opaque.

---

## Step 3 ‚Äî Manual annotation with SMEs (2‚Äì3 minutes)

**What you show**

* Annotation fields (pass / fail, error type, comments)

**What you say**

> SMEs can annotate answers directly:
>
> * mark failure modes (e.g. hallucination, omission)
> * flag unacceptable behavior
> * add short comments explaining why
>
> This turns expert judgment into a reusable evaluation artifact.

**Key message**

* SME knowledge is **captured**, not lost in meetings.

---

## Step 4 ‚Äî Automated evaluation & scoring (2‚Äì3 minutes)

**What you show**

* Aggregate scores
* Pass/fail counts
* Error distribution by type

**What you say**

> Once acceptance criteria are defined, Evaluizer can run automated checks:
>
> * deterministic rules
> * LLM-based judges
> * or combinations of both
>
> These scores don‚Äôt replace human judgment ‚Äî they make changes *visible* and *comparable* over time.

**Key message**

* Evaluation is **repeatable**, not subjective every time.

---

## Step 5 ‚Äî Comparing changes over time (2‚Äì3 minutes)

**What you show**

* Comparison between:

  * two prompt versions
  * or before/after a data change

**What you say**

> This is where evaluation really pays off.
>
> When something changes ‚Äî a prompt, a document set, a model ‚Äî you can immediately see:
>
> * what improved
> * what regressed
> * whether risk increased or decreased
>
> This prevents silent degradation.

**Key message**

* No more ‚Äúit seemed better yesterday‚Äù.

---

## Step 6 ‚Äî Handover & continuity (2 minutes)

**What you show**

* Clean project structure
* Saved evaluation sets
* Clear naming/versioning

**What you say**

> At the end of the engagement, this setup is handed over:
>
> * your evaluation dataset
> * your acceptance criteria
> * your evaluation workflow
>
> Internal teams can continue running evaluations without my involvement.
>
> This avoids vendor lock-in and reduces long-term risk.

**Key message**

* You are **not dependent** on the consultant.

---

## What you explicitly do NOT demo

* Model internals
* Embeddings
* Vector databases
* Prompt ‚Äúcleverness‚Äù
* Optimization tricks

If asked, you say:

> Those choices only matter *after* evaluation criteria are clear.

---

## Typical stakeholder reactions (and how to respond)

### ‚ÄúThis looks slower than just building something‚Äù

> It‚Äôs slower than a demo ‚Äî and much faster than recovering trust after a failed deployment.

### ‚ÄúCan our team use this later?‚Äù

> Yes. The evaluation setup is explicitly designed for internal continuation.

### ‚ÄúDoes this guarantee correctness?‚Äù

> No system can guarantee correctness. This makes risks explicit and measurable.

---

## Positioning sentence (use verbatim)

> Evaluizer turns evaluation from a vague promise into a concrete, inspectable workflow that aligns AI behavior with domain risk.

---

## Consultant‚Äôs internal note

This demo:

* reassures risk-averse stakeholders
* filters out ‚Äúmove fast‚Äù clients
* reinforces your evaluation-first positioning
* justifies your fee without talking about price

---

---

# 15-Minute Intro Call Script

**Evaluation-First Knowledge System Assessment & Prototype**

## Goal of the call (internal)

* Qualify fast: **is this a good fit** and **is there a real problem**
* Align expectations: **not a chatbot demo**, **not production**
* Secure next step: either **SME workshop** or **polite no**

## Tone

Calm, structured, senior. No hype.

---

## 0:00‚Äì0:45 ‚Äî Opening

**You say:**

> Thanks for taking the time. To make this useful, I suggest a short structure:
> I‚Äôll briefly explain how I work (2 minutes), then I‚Äôll ask a few questions about your context (8‚Äì9 minutes), and we‚Äôll close with a clear next step (2‚Äì3 minutes). Does that work?

*(If they say yes, proceed. If they go off-track early, gently return to the structure.)*

---

## 0:45‚Äì2:30 ‚Äî What you do (your positioning, short)

**You say:**

> I help organizations design **AI-assisted knowledge systems that can be meaningfully evaluated and governed before deployment**.
> Most GenAI efforts fail because success criteria and failure modes are not defined in a domain-specific way.
> My approach is evaluation-first: we co-design acceptance criteria and an evaluation set with SMEs, then build a small instrumented prototype to test feasibility and risk.
> The deliverable is a clear decision: proceed, iterate, or stop ‚Äî with evidence.

**Optional (if they ask ‚Äúwhy you?‚Äù):**

> My background is in data science, governance/decision-making, and cybersecurity in regulated environments ‚Äî so I‚Äôm conservative by design.

---

## 2:30‚Äì11:30 ‚Äî Qualification questions (pick 6‚Äì8, don‚Äôt rush)

### A) Use case & decisions (must-have)

1. **Decision:**

> What decision(s) would this system support? What changes if the answer is good?

2. **Users:**

> Who would use it day-to-day, and who would be accountable for outcomes?

3. **Current workflow pain:**

> How do people solve this today ‚Äî searching, asking colleagues, reinventing, manual review?

### B) Risk & failure tolerance (must-have)

4. **Cost of being wrong:**

> What happens if the system gives a confident but incorrect answer?

5. **Refusal design:**

> Are there question types where the system should *never* answer and must refuse?

6. **Auditability / explainability:**

> Do you need citations, traceability to documents, or an audit trail?

### C) Data reality check (must-have)

7. **Data types:**

> What documents are we talking about (policies, procedures, reports, tickets, emails, contracts, etc.)?

8. **Access constraints:**

> Any restrictions around where the data can be processed (on-prem only, CH/EU cloud only, strict access controls)?

### D) Organizational readiness (nice-to-have)

9. **SME availability:**

> Who are the 2‚Äì4 SMEs who could join a structured evaluation workshop?

10. **Success criteria:**

> If we did this well, what would you want to see at the end of 6‚Äì8 weeks?

**Your rule:** If they can‚Äôt name SMEs or a decision owner, this is not ready.

---

## 11:30‚Äì13:30 ‚Äî Reflect back & frame the fit

**You say (summarize in 20‚Äì30 seconds):**

> Let me reflect back what I heard:
> You want to support [DECISION] for [USERS], using [DATA], with constraints around [GOV/SEC], and the main risk is [RISK]. Is that accurate?

Then:

**If fit is good:**

> This sounds like a good fit for an evaluation-first assessment. The next step would be a structured SME workshop to define acceptance criteria, failure modes, and an initial evaluation set.

**If fit is weak:**

> Based on what you described, I‚Äôm not sure this is the right time to build anything yet. The missing piece is [SMEs / data access / decision ownership]. If you address that, we can revisit.

---

## 13:30‚Äì15:00 ‚Äî Close with a concrete next step

### Option 1: Schedule the SME Evaluation Workshop (best)

**You say:**

> If you‚Äôre open to it, I suggest a 90‚Äì120 minute workshop with 2‚Äì4 SMEs and a process owner.
> Pre-work is light: 5‚Äì10 sample documents and 5‚Äì10 real questions people ask today.
> After the workshop, I can confirm scope and deliverables precisely.

### Option 2: Request a small data sample first (when access is unclear)

**You say:**

> If workshop scheduling is hard right now, we can start with a small document sample and a question list. I‚Äôll review it and propose a workshop agenda tailored to your domain.

### Option 3: Decline politely (protect your time)

**You say:**

> I don‚Äôt want to waste your time. Without [SME availability / data access / decision owner], this will likely become a demo without evaluation rigor. If you can align those pieces, I‚Äôd be happy to pick it up again.

---

# What you do NOT say (common traps)

* Don‚Äôt promise ‚Äú95% accuracy‚Äù or any number.
* Don‚Äôt say ‚Äúwe‚Äôll eliminate hallucinations.‚Äù
* Don‚Äôt say ‚Äúenterprise-grade‚Äù unless asked; show it via method.
* Don‚Äôt offer operations, uptime, maintenance.
* Don‚Äôt discuss tools/models early (Evaluizer, embeddings, etc.) unless they ask.

---

# If they ask about Evaluizer (30-second answer)

**You say:**

> I use an evaluation interface so SMEs can review outputs against an evaluation dataset, annotate results, and run repeatable checks over time.
> It makes the evaluation process concrete and helps internal teams continue evaluation after the engagement.

*(Stop there. Offer a demo later, not now.)*

---

# Quick red flags (end early if 2+ occur)

* ‚ÄúWe just want a chatbot quickly‚Äù
* No SME time available
* No data access path
* No clear decision owner
* They want production deployment + support from you
* They insist on vendor-like guarantees

---

# Success criteria for this call (internal)

* ‚úÖ Identified decision owner + SMEs
* ‚úÖ Understood risk tolerance and refusal needs
* ‚úÖ Confirmed data types + constraints
* ‚úÖ Agreed next step: SME workshop (preferred) or data sample review

---

---

# Proposal

**Evaluation-First Knowledge System Assessment & Prototype**

**Client:** [Organization name]
**Date:** [Date]
**Prepared by:** Fatih √únal

---

## 1. Context & Objective

[Organization] is exploring the use of **AI-assisted knowledge systems** to support decisions related to:

* **Decision(s):** [brief description]
* **Users:** [roles / functions]
* **Primary risk:** [e.g. incorrect, incomplete, or outdated answers]

The objective of this engagement is **not to deploy a production system**, but to determine **whether such a system can be trusted in this domain**, and under which conditions.

---

## 2. Why an evaluation-first approach

Industry experience shows that most GenAI initiatives fail in practice because:

* success criteria are implicit or undefined
* evaluation does not reflect real decision risk
* failure modes are discovered too late
* there is no mechanism for ongoing assessment

This engagement is designed around a different principle:

> **If a system cannot be meaningfully evaluated in its domain context, it should not be deployed.**

---

## 3. Scope of the engagement (6‚Äì8 weeks)

### 3.1 Use-case & risk framing

* Clarify supported decisions and accountability
* Identify unacceptable vs tolerable failure modes
* Define when the system must refuse to answer

**Deliverable:**
Problem & risk framing document

---

### 3.2 Data & governance readiness

* Review document types and structure
* Assess sensitivity, access constraints, and audit requirements
* Identify data gaps and no-go signals

**Deliverable:**
Data readiness & governance memo

---

### 3.3 Joint evaluation & system design (with SMEs)

Structured working sessions with Subject Matter Experts to define:

* domain-specific success and failure criteria
* error taxonomy and refusal rules
* acceptance thresholds aligned to organizational risk
* initial evaluation dataset (SME-authored or validated)

**Deliverables:**

* Evaluation design document
* Initial evaluation dataset

---

### 3.4 Evaluation-guided prototype

Implementation of a **limited-scope, instrumented prototype** to:

* run batch evaluations against agreed criteria
* analyze failure modes and trade-offs
* support regression testing as prompts or data change

The prototype serves as a **test harness**, not a production system.

**Deliverables:**

* Instrumented prototype
* Evaluation report with failure analysis

---

### 3.5 Decision memo

A structured recommendation covering:

* feasibility against acceptance criteria
* remaining risks and constraints
* effort required to proceed
* recommendation to proceed, iterate, or stop

**Deliverable:**
Decision memo suitable for management and governance review

---

## 4. Evaluation infrastructure

As part of the engagement, an **evaluation interface and workflow** is provided based on *Evaluizer*, an open-source framework for systematic LLM evaluation.

This enables:

* transparent review of model outputs against evaluation datasets
* manual annotation by SMEs
* automated checks using deterministic functions and LLM judges
* continued evaluation after the engagement concludes

The tooling is configured for the specific use case and handed over as part of the delivery.

---

## 5. Engagement model

* Fixed scope
* Fixed duration (6‚Äì8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

Work is conducted independently using client-provided data and environments.

---

## 6. Roles & expectations

**Client provides:**

* access to representative documents
* availability of 2‚Äì4 SMEs for evaluation workshops
* a decision owner for scoping and sign-off

**Consultant provides:**

* evaluation-first methodology
* facilitation of SME workshops
* prototype and evaluation artifacts
* independent recommendation

---

## 7. Commercials

**Fee:** CHF [XX‚Äô000] (fixed)
**Payment terms:** [e.g. 50% start / 50% end]
**Start date:** [date]

No licenses or recurring fees.

---

## 8. Independence & IP

All work is conducted independently and uses only client-provided or publicly available materials.
No confidential or proprietary information of third parties is reused or derived.

---

## 9. Next step

Upon confirmation, the engagement starts with scheduling the **SME Evaluation Workshop**, followed by document review and detailed planning.

---

### Consultant note (internal)

This proposal is intentionally conservative.
If a client pushes for speed, production, or guarantees ‚Üí **decline or re-scope**.

---

## What we‚Äôve achieved so far

At this point you have:

* a **clear service**
* a **repeatable sales call**
* a **proposal that matches the call**
* consistent language across all artifacts

This is already a complete consulting ‚Äúmachine‚Äù.

---

## Next (final artifact in this set)

üëâ **`05_tools_evaluizer/evaluizer_demo_flow.md`**
A short, non-technical demo flow you can:

* show in follow-up calls
* include screenshots from later
* use to reduce fear around ‚Äúmaintenance after you leave‚Äù

When you‚Äôre ready, say:

> ‚ÄúProceed with the Evaluizer demo flow.‚Äù

We‚Äôll keep it tight and usable.

# 2Ô∏è‚É£ One-Pager

## ‚ÄúWhy 95% of GenAI Systems Fail ‚Äî and how to be in the 5%‚Äù

This is **not** a blog post.
It‚Äôs a conversation anchor.

You send it:

* before intro calls
* to skeptical stakeholders
* to decision-makers who are nervous but curious

---

### Headline

**Why 95% of GenAI Systems Fail in Practice**

*And what successful organizations do differently*

---

### The uncomfortable reality

Most GenAI systems fail not because the models are weak, but because organizations deploy them **without defining what success and failure actually mean**.

Typical symptoms:

* impressive demos that collapse in real use
* pilot fatigue and loss of trust
* hidden risks discovered too late
* inability to explain or defend system behavior

---

### The five root causes of failure

1. **No explicit success criteria**
   ‚ÄúIt seems good‚Äù is not an evaluation strategy.

2. **No domain-grounded evaluation data**
   Systems are tested on generic prompts, not real decisions.

3. **No error taxonomy**
   All mistakes are treated as equal ‚Äî until one causes damage.

4. **No refusal or uncertainty design**
   Systems answer questions they should not answer.

5. **No way to measure improvement over time**
   Changes are made blindly, without regression testing.

---

### What the 5% do differently

Successful GenAI systems are designed around **evaluation first**:

* domain experts define acceptable and unacceptable behavior
* risks are classified before systems are built
* failure modes are made explicit
* evaluation datasets exist before optimization
* systems are tested continuously as data and prompts change

In other words:
**They treat GenAI as a decision system, not a demo.**

---

### The key principle

> If a system cannot be evaluated in a way that reflects real-world risk, it should not be deployed.

---

### What this means in practice

Before building anything, organizations should be able to answer:

* What does a *bad* answer look like?
* When must the system refuse?
* Which errors are unacceptable?
* Who remains accountable?

If these questions cannot be answered, no amount of model tuning will help.

---

### Why this matters

Evaluation-first design:

* reduces deployment risk
* prevents reputational damage
* saves time and cost by avoiding false starts
* builds trust with users and stakeholders

It is slower upfront ‚Äî and dramatically faster overall.

---

### Closing line (soft CTA)

If your organization is under pressure to ‚Äúuse GenAI‚Äù, but unsure how to do so responsibly, an evaluation-first assessment can clarify whether this is the right path ‚Äî and under what conditions.

---

## Consultant‚Äôs closing assessment

You now have:

* a **distinctive core offer**
* a **credible evaluation methodology**
* a **tangible SME interaction**
* a **clear narrative explaining failure and success**
* a **tool (Evaluizer) that supports continuity without locking clients in**

This is not ‚ÄúAI consulting‚Äù.
This is **decision systems consulting with modern tools**.

---

## What I would do next (optional but powerful)

When you‚Äôre ready, the next logical artifacts are:

1. **A 15-minute intro call script** (what you say, what you don‚Äôt)
2. **A 1-page proposal template** using the workshop + assessment
3. **A short Evaluizer demo flow** (screenshots + explanation)

But pause here if you need ‚Äî this is already a *solid, coherent product*.

If you want to continue, tell me which of the three you want next.
