
---

# â€œDerived from /01_services/services_page.md (canonical)â€

# Services (Evaluation-First Version, Publication-Ready)

## Evaluation-First Knowledge System Assessment & Prototype

### What this service is

I help organizations **design AI-assisted knowledge systems that can be meaningfully evaluated, governed, and trusted before they are deployed**.

Most GenAI initiatives fail not because the models are weak, but because *success is never defined in a way that reflects real organizational risk*. This service addresses that gap by placing **evaluation and decision quality** at the center of system design.

The outcome is not a demo chatbot, but a **clear, evidence-based decision** on whether such a system should exist in your context â€” and under what constraints.

---

### Why this approach

Industry experience shows that the majority of GenAI pilots fail in production due to:

* undefined or implicit success criteria
* lack of domain-specific evaluation data
* no error taxonomy or acceptance thresholds
* inability to reason about failure modes
* missing human-in-the-loop design

This service is built around the opposite principle:

> **If a system cannot be evaluated in a way that reflects domain risk, it should not be deployed.**

---

### Who this service is for

This service is suited for organizations where:

* decisions depend on complex internal documents or policies
* incorrect answers have legal, financial, or reputational consequences
* AI systems must be auditable and explainable
* data use is constrained by regulation or governance
* there is pressure to â€œuse GenAIâ€, but uncertainty about how to do so responsibly

Typical contexts include public institutions, regulated organizations, NGOs, and research-driven environments.

---

### Who this service is *not* for

This service is not intended for organizations seeking:

* a ready-made SaaS chatbot
* customer support or sales automation
* real-time or mission-critical AI systems
* rapid prototyping without evaluation rigor
* ongoing system operations or maintenance

If speed matters more than correctness, this engagement is not a good fit.

---

## Scope & approach (Evaluation-first)

The engagement is structured as a **time-boxed assessment and prototype**, typically over **6â€“8 weeks**, and is organized around evaluation rather than implementation speed.

---

### 1. Use-case & risk framing

We begin by clarifying:

* which decisions the system would support
* what constitutes a *wrong* or *unacceptable* answer
* which failure modes are tolerable vs unacceptable
* where the system must refuse to answer
* who remains accountable for outcomes

This step anchors the entire evaluation design.

**Deliverable:**
Problem & risk framing document

---

### 2. Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* data quality and coverage gaps
* governance and audit requirements

This step often surfaces reasons *not* to proceed â€” which is considered a valid outcome.

**Deliverable:**
Data readiness & governance memo (including go / no-go signals)

---

### 3. Joint evaluation & system design (with SMEs)

This is the core differentiator of the engagement.

In structured working sessions with Subject Matter Experts and process owners, we jointly design:

* task-specific evaluation datasets
* domain-relevant success and failure criteria
* error taxonomies (hallucination, omission, ambiguity, outdated information, etc.)
* acceptance thresholds aligned to organizational risk
* human-in-the-loop and override rules

Evaluation design precedes system optimization.

**Deliverables:**

* Evaluation design document
* Initial SME-validated evaluation set

---

### 4. Evaluation-guided prototype

A limited-scope prototype is implemented **as a test harness**, not as a production system.

The prototype is instrumented to:

* run batch evaluations
* inspect retrieval and answer quality
* analyze failure modes
* support regression testing as data or prompts change

The goal is to make failures visible, inspectable, and discussable.

**Deliverables:**

* Instrumented prototype
* Evaluation report with failure analysis and trade-offs

---

### 5. Decision memo

The engagement concludes with a structured decision memo covering:

* whether the system meets acceptance criteria
* what would be required to reach them
* which risks remain irreducible
* whether to proceed, iterate, or stop

**Deliverable:**
Decision memo suitable for internal governance and management review

---

## Evaluation infrastructure & continuity

As part of the engagement, I provide an **evaluation interface and workflow** based on *Evaluizer*, an open-source evaluation framework designed for systematic LLM assessment and optimization.

This allows organizations to:

* visualize model outputs against evaluation datasets
* manually annotate and review results with SMEs
* run automated evaluations using deterministic functions and LLM judges
* track performance over time as prompts, data, or models change

The goal is to **reduce fear around long-term maintenance** and enable internal teams to continue evaluation after the engagement ends.

Evaluizer is adapted and configured for the specific use case and handed over as part of the delivery.

---

## Engagement model

* Fixed scope
* Fixed duration (6â€“8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

Engagements are conducted independently using client-provided data and environments, with strict separation from any employer-related activities.

---

## Independence & IP

All engagements use only client-provided or publicly available materials.
I ensure strict separation from any employer-related activities and do not reuse or derive from confidential or proprietary information of third parties.

---

## Next step

If you are considering an AI-assisted knowledge system and want to determine **whether it can be trusted in your domain**, an initial discussion can clarify whether this engagement is appropriate.

---

# Consultantâ€™s assessment of this positioning

This rewrite does three important things:

1. **Evaluation is no longer a feature â€” it is the product**
2. You sell *decision safety*, not model performance
3. Evaluizer is positioned as **risk-reducing infrastructure**, not a shiny tool

This is exactly how you avoid:

* academic overtones
* tool-centric selling
* being mistaken for an â€œAI dev shopâ€

---

# What we do next (as requested)

Now that the Services page is coherent, the next two artifacts should be:

### Next 1ï¸âƒ£ SME Workshop Agenda (sales + delivery artifact)

A concrete, 90â€“120 minute agenda you can:

* attach to proposals
* walk through in intro calls
* use to show seriousness without code

### Next 2ï¸âƒ£ â€œWhy 95% of GenAI Systems Failâ€ One-Pager

A short, non-technical explainer you can:

* send before first calls
* use internally with skeptical stakeholders
* anchor your evaluation-first narrative

Both are **high-leverage** and low effort.

---

**My recommendation:**
ðŸ‘‰ Next, we design the **SME Evaluation Workshop agenda**, because it makes the service tangible and dramatically increases conversion quality.

If you agree, say:

> â€œLetâ€™s design the SME workshop agenda.â€

Weâ€™ll do it at the same level of rigor as everything else.

---

# `01_services/services_page.md`

**FINAL â€“ Canonical, Evaluation-First Version**

---

# Services

## Evaluation-First Knowledge System Assessment & Prototype

---

## What this service is

I help organizations design **evaluation-first AI-assisted knowledge systems** that can be meaningfully evaluated, governed, and trusted **before deployment**.

The focus is not on building chatbots or pilots, but on defining **acceptance criteria, failure modes, and decision risk** before any system is operationalized.

The outcome is a **clear, evidence-based decision** on whether such a system should exist in your context â€” and under what constraints.

---

## The problem this service addresses

Most GenAI initiatives fail not because models are weak, but because **success and failure are never defined in a way that reflects real organizational risk**.

As a result:

* systems perform well in demos but fail in real use
* unacceptable errors are discovered too late
* refusal and uncertainty are not designed
* trust erodes quickly

This service is built on the opposite principle:

> **If a system cannot be meaningfully evaluated in its domain context, it should not be deployed.**

---

## Who this service is for

This service is suited for organizations where:

* decisions depend on complex internal documents
* incorrect or misleading answers have legal, financial, or reputational consequences
* explainability, auditability, and governance matter
* data usage is constrained by regulation or policy

Typical contexts include regulated organizations, public institutions, NGOs, and research-driven environments.

---

## Who this service is not for

This service is **not** intended for organizations seeking:

* a ready-made SaaS product
* customer support or sales chatbots
* rapid experimentation or demo-driven pilots
* real-time or mission-critical systems
* ongoing system operation or maintenance

If speed matters more than correctness, this engagement is not a good fit.

---

## Scope & approach (evaluation-first)

The engagement is structured as a **fixed-scope, time-boxed assessment** (typically **6â€“8 weeks**) and follows a strict evaluation-first sequence.

---

### 1. Use-case & risk framing

We clarify:

* which decisions the system would support
* who remains accountable for outcomes
* what constitutes unacceptable vs tolerable failure
* where the system must refuse to answer

**Deliverable**
Problem & risk framing document

---

### 2. Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* audit and governance requirements
* data gaps and no-go signals

A recommendation **not to proceed** is considered a valid outcome.

**Deliverable**
Data readiness & governance memo

---

### 3. Joint evaluation & system design (with SMEs)

This is the core of the engagement.

In structured working sessions, **Subject Matter Experts co-define**:

* domain-specific acceptance criteria
* failure modes and refusal rules
* evaluation datasets reflecting real decision risk

Evaluation design precedes system optimization.

**Deliverables**

* Evaluation design document
* SME-validated evaluation dataset

---

### 4. Evaluation-guided prototype

A limited-scope prototype is implemented as an **instrumented test harness**, not a production system.

It is used to:

* run evaluations against agreed criteria
* inspect failure modes and trade-offs
* identify irreducible risks

**Deliverables**

* Instrumented prototype
* Evaluation report with failure analysis

---

### 5. Decision memo

The engagement concludes with a structured recommendation to:

> **Proceed Â· Iterate Â· Stop**

**Deliverable**
Decision memo suitable for management and governance review

---

## Engagement success criteria

This engagement is successful if, at the end of the assessment, the organization has:

* a documented evaluation framework grounded in domain risk
* a prototype tested against SME-defined acceptance criteria
* a clear, defensible decision to proceed, iterate, or stop

No production deployment is required for success.

---

## Evaluation infrastructure & continuity

An evaluation interface based on **Evaluizer** is used during the engagement and handed over so internal teams can continue evaluation independently.

This enables:

* transparent inspection of system behavior
* repeatable evaluation over time
* regression detection as data or prompts change

No vendor lock-in is introduced.

---

## Engagement model

* Fixed scope
* Fixed duration (6â€“8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

**This engagement is intentionally conservative and is not designed for rapid experimentation or production deployment.**

---

## Time commitment

* SME involvement is typically limited to **2â€“4 hours total**, primarily through a structured evaluation workshop
* One decision owner is required for scoping and sign-off

---

## Independence & IP

All work is conducted independently and uses only client-provided or publicly available materials.
No confidential or proprietary information of third parties is reused or derived.

---

## If the recommendation is to stop

If the recommendation is not to proceed, **all evaluation artifacts, decision frameworks, and findings remain usable** for future initiatives, audits, or internal reviews.

Stopping is treated as a valid and valuable outcome.

---

## Next step

If you are considering an AI-assisted knowledge system and want to determine **whether it can be trusted in your domain**, an initial discussion can clarify whether this engagement is appropriate.

---


---

# Evaluation-First Principles

## How trustworthy AI-assisted knowledge systems are designed

---

## Principle 1 â€” Evaluation precedes optimization

AI-assisted knowledge systems must be evaluated **before** they are optimized.

In practice, many systems are tuned for fluency or apparent accuracy without a clear definition of what constitutes acceptable behavior. This leads to systems that perform well in demonstrations but fail in real decision contexts.

Experienced practitioners consistently emphasize that **optimization without prior evaluation is meaningless**, because there is no stable reference for improvement [1].

Evaluation therefore acts as the **specification layer** of the system, not a validation step applied at the end.

---

## Principle 2 â€” Evaluation must be domain-specific

Generic benchmarks and abstract accuracy scores are insufficient for real-world use.

What matters is whether system behavior is acceptable **within a specific domain**, under real constraints, for concrete decisions. Acceptability cannot be inferred from model performance alone.

Applied AI research and industry practice show that **task- and domain-specific evaluation** is required to surface meaningful failure modes and prevent silent errors [2][3].

---

## Principle 3 â€” Subject Matter Experts define acceptability

Only Subject Matter Experts (SMEs) can define:

* what constitutes an acceptable answer
* which errors are tolerable
* which failures are unacceptable regardless of frequency

SMEs are not asked to evaluate model internals.
They define **decision risk and acceptance criteria**.

Human-in-the-loop and expert-grounded evaluation approaches are widely recognized as necessary in high-stakes and regulated settings, where correctness is contextual rather than purely statistical [3][4].

---

## Principle 4 â€” Refusal is a designed behavior, not a failure

In many domains, the safest answer is **no answer**.

Systems must be explicitly designed to:

* refuse questions outside their scope
* abstain when confidence is insufficient
* signal uncertainty instead of hallucinating

Refusal behavior is a **risk control mechanism**, not a deficiency.
Leading AI labs emphasize that evaluation must include when and how systems should *not* respond [5].

---

## Principle 5 â€” Evaluation artifacts are first-class system components

Evaluation artifacts include:

* acceptance criteria
* evaluation datasets
* failure taxonomies
* annotated system behaviors

These artifacts are not temporary scaffolding. They are reusable system components that:

* support governance and audit
* enable regression detection
* retain value even if deployment is deferred or canceled

For this reason, evaluation artifacts are treated as **deliverables**, not byproducts.

---

## Principle 6 â€” Decisions are evidence-based, not aspirational

The purpose of evaluation is not to justify deployment.

The outcome of an evaluation-first process is a structured decision to:

> **Proceed Â· Iterate Â· Stop**

All three outcomes are valid.
Stopping based on evidence is considered a successful outcome.

This principle aligns with emerging best practices in AI governance, which emphasize decision traceability over optimistic experimentation [1][2].

---

## Summary

An evaluation-first approach shifts AI system design from:

* model-centric thinking
* demo-driven validation
* post-hoc risk discovery

to:

* domain-anchored evaluation
* explicit failure design
* evidence-based deployment decisions

This discipline is what separates AI systems that merely impress from systems that can be trusted.

---

## References

[1] Hamel Husain â€” *Evals Before Optimization*
[https://hamel.dev/blog/posts/evals-faq/](https://hamel.dev/blog/posts/evals-faq/)

[2] Anthropic â€” *A Statistical Approach to Model Evaluations*
[https://www.anthropic.com/research/statistical-approach-to-model-evals](https://www.anthropic.com/research/statistical-approach-to-model-evals)

[3] Eugene Yan â€” *Productizing Machine Learning* / *ML System Failure Modes*
[https://eugeneyan.com/writing/](https://eugeneyan.com/writing/)

[4] Stanford Human-Centered AI â€” Evaluation in High-Stakes Domains
[https://hai.stanford.edu/research](https://hai.stanford.edu/research)

[5] OpenAI â€” Evaluation & System Behavior Guidance
[https://platform.openai.com/docs/guides/evals](https://platform.openai.com/docs/guides/evals)

---

---

# Failure Taxonomy

## How unacceptable system behavior is identified, classified, and evaluated

---

## Purpose of this document

This document defines a **failure taxonomy** for evaluation-first AI-assisted knowledge systems.

The taxonomy serves three purposes:

1. Provide a **shared vocabulary** for SMEs, engineers, and decision owners
2. Enable **systematic evaluation** beyond generic accuracy metrics
3. Support **governance, audit, and decision traceability**

Failures are not treated as bugs to be hidden, but as **signals that determine whether deployment is acceptable**.

---

## Core principle

> A system does not fail when it is wrong.
> A system fails when it behaves **unacceptably for its domain and risk context**.

This taxonomy is therefore **domain-agnostic in structure**, but **domain-specific in application**.

---

## Failure categories (canonical)

All system failures must be classified into **exactly one primary category**.
Secondary categories may be noted, but one must dominate.

---

## 1. Hallucination

**Category:** Critical

### Definition

The system produces information that is **not supported by the provided documents** or invents facts, entities, or relationships.

### Why this matters

Hallucination erodes trust immediately and is often indistinguishable from correct answers for non-expert users.

### Examples

* Citing a regulation that does not exist
* Inventing a policy clause
* Attributing statements to documents that do not contain them

### Evaluation rule

* **Always unacceptable** in high-stakes or regulated contexts
* Requires explicit detection and mitigation

---

## 2. Unsupported inference

**Category:** Critical

### Definition

The system draws conclusions that are **not explicitly supported** by the source material, even if they sound plausible.

### Why this matters

This is more subtle than hallucination and often goes unnoticed.

### Examples

* Inferring intent or legal meaning not stated in the text
* Combining unrelated facts into a new conclusion

### Evaluation rule

* Typically unacceptable
* SMEs must decide whether bounded inference is ever permitted

---

## 3. Omission

**Category:** High

### Definition

The system provides an answer that is technically correct but **misses critical information** required for a safe or complete decision.

### Why this matters

Partial answers can be more dangerous than incorrect ones.

### Examples

* Summarizing a policy while omitting key exceptions
* Answering a procedural question without mentioning mandatory approvals

### Evaluation rule

* Often unacceptable in decision-support contexts
* Acceptability depends on domain and task

---

## 4. Overconfidence

**Category:** High

### Definition

The system presents uncertain or ambiguous information with **unwarranted confidence**.

### Why this matters

Users tend to trust confident language, even when uncertainty should be explicit.

### Examples

* Definitive answers where documents are ambiguous
* Failure to signal uncertainty or assumptions

### Evaluation rule

* Unacceptable when uncertainty materially affects decisions
* Must be paired with uncertainty signaling or refusal

---

## 5. Refusal failure

**Category:** Critical

### Definition

The system answers a question that it **should have refused** based on scope, confidence, or policy constraints.

### Why this matters

This is a design failure, not a model limitation.

### Examples

* Answering questions outside the document corpus
* Responding to requests requiring human judgment or authority

### Evaluation rule

* Always unacceptable
* Refusal behavior must be explicitly tested

---

## 6. Over-refusal

**Category:** Medium

### Definition

The system refuses to answer questions it **should be able to answer** safely and correctly.

### Why this matters

Excessive refusal degrades usability and trust.

### Examples

* Refusing clearly supported factual questions
* Overly conservative abstention

### Evaluation rule

* Acceptable during early iterations
* Must be monitored and reduced deliberately

---

## 7. Misalignment with domain norms

**Category:** Medium

### Definition

The system produces answers that are factually correct but **inappropriate in tone, framing, or interpretation** for the domain.

### Why this matters

Domain norms encode risk, responsibility, and professionalism.

### Examples

* Legal answers phrased as informal advice
* Policy interpretations framed as recommendations

### Evaluation rule

* SMEs define acceptability
* Often unacceptable in formal or regulated settings

---

## 8. Source attribution failure

**Category:** High

### Definition

The system fails to correctly reference or attribute the source material used in its answer.

### Why this matters

Traceability is essential for audit, trust, and verification.

### Examples

* Missing citations
* Incorrect document references
* Blended sources without distinction

### Evaluation rule

* Often unacceptable where auditability is required

---

## Severity mapping (guidance)

| Category                   | Typical Severity |
| -------------------------- | ---------------- |
| Hallucination              | Critical         |
| Unsupported inference      | Critical         |
| Refusal failure            | Critical         |
| Omission                   | High             |
| Overconfidence             | High             |
| Source attribution failure | High             |
| Misalignment with norms    | Medium           |
| Over-refusal               | Medium           |

Final severity classification is **domain-dependent** and decided by SMEs.

---

## How this taxonomy is used in evaluation

During evaluation:

* Each answer is annotated with:

  * failure category (if any)
  * severity
  * short SME comment
* Aggregated results reveal:

  * dominant failure modes
  * irreducible risks
  * suitability for deployment

This enables **transparent trade-off analysis**, not binary pass/fail scoring.

---

### Operationalization note

This failure taxonomy defines *what* constitutes unacceptable behavior.  
How failures are recorded, annotated, and aggregated is described separately in the evaluation methodology and tooling documentation.

The taxonomy is intentionally tool-agnostic.

---

## Governance & audit relevance

This taxonomy:

* supports explainable deployment decisions
* documents why a system was approved, constrained, or rejected
* provides evidence for internal and external audits

Failure classification is therefore a **governance artifact**, not a technical detail.

---

## Summary

A trustworthy AI-assisted knowledge system is not defined by how often it answers correctly, but by **how it fails**.

This taxonomy ensures failures are:

* visible
* classified
* discussed
* and designed against

Without it, evaluation degenerates into anecdote.

---

## Related documents

This taxonomy is used in conjunction with:

- `evaluation_first_principles.md` â€” conceptual foundation of evaluation-first design  
- `sme_workshop_agenda.md` â€” how SMEs define acceptance criteria and failure modes  
- `evaluation_design_framework.md` â€” how evaluation datasets and criteria are constructed  
- `evaluizer_overview.md` â€” tooling used to operationalize evaluation during the engagement

---
---

# 1ï¸âƒ£ SME Evaluation Workshop Agenda

**â€œFrom domain knowledge to evaluable system designâ€**

### Purpose (how you explain it to clients)

> This workshop translates domain expertise into concrete evaluation criteria, so that an AI-assisted knowledge system can be assessed meaningfully before deployment.

You are not â€œbrainstorming promptsâ€.
You are **formalizing judgment**.

---

## Workshop at a glance

* **Duration:** 90â€“120 minutes
* **Participants:**

  * 2â€“4 Subject Matter Experts
  * 1 process owner / decision-maker
  * (Optional) compliance / risk representative
* **Format:** Structured, facilitated session
* **Output:** Evaluation design inputs, not technical implementation

---

## Pre-work (asynchronous, very light)

You request **before the workshop**:

* 5â€“10 representative documents
* 5â€“10 example questions people *already ask*
* A short description of decisions these answers inform

This filters unserious engagements early.

---

## Workshop Agenda (Detailed)

---

### 0. Context setting (10 minutes)

**Goal:** Align expectations and psychological safety

You explain:

* This is *not* about building a chatbot
* Wrong answers matter more than impressive answers
* Refusal to answer is a valid outcome
* Evaluation comes before optimization

Key sentence you say out loud:

> â€œIf we canâ€™t define what â€˜goodâ€™ looks like, we shouldnâ€™t automate it.â€

---

### 1. Decision & risk mapping (20â€“25 minutes)

**Goal:** Anchor evaluation in real consequences

You facilitate answers to:

* What decisions would this system support?
* Who relies on these answers?
* What happens if the answer is wrong?
* What happens if the answer is incomplete?
* What happens if the answer sounds confident but is outdated?

You explicitly surface:

* *high-risk vs low-risk questions*
* *where human judgment must remain*

**Output:**

* Decision categories
* Risk tiers (e.g. unacceptable / tolerable / informational)

---

### 2. Failure mode identification (20â€“25 minutes)

**Goal:** Make errors explicit and discussable

You guide SMEs through a **failure taxonomy**, such as:

* hallucination (fabricated facts)
* omission (missing critical constraints)
* ambiguity (multiple interpretations)
* outdated information
* misapplied policy
* overconfident tone
* inappropriate refusal

You ask:

* Which of these are unacceptable?
* Which are tolerable?
* Which should trigger refusal?

**Output:**

* Domain-specific error taxonomy
* Initial refusal rules

This is where trust is built.

---

### 3. Defining â€œacceptable answersâ€ (20â€“25 minutes)

**Goal:** Move beyond vague notions of correctness

You work with SMEs to define:

* what *must* be present in a good answer
* what *must never* be present
* when uncertainty should be expressed
* when sources must be cited or constrained

You avoid accuracy percentages.
You talk about **answer properties**.

**Output:**

* Acceptance criteria
* Qualitative scoring dimensions

---

### 4. Evaluation dataset design (20 minutes)

**Goal:** Turn expertise into test cases

Together, you define:

* representative questions per risk tier
* edge cases and â€œtrap questionsâ€
* examples that should trigger refusal
* examples where partial answers are acceptable

You explicitly state:

> â€œThis is not training data. This is test data.â€

**Output:**

* Initial evaluation question set
* SME-validated expected behaviors

---

### 5. Handover & next steps (10 minutes)

**Goal:** Close cleanly and confidently

You explain:

* how these outputs feed into the prototype
* how Evaluizer will be used to run and inspect evaluations
* what will be tested and what will not
* how results will be reviewed together

You do *not* promise performance.

---

## Workshop Deliverables (what you list in proposals)

* Decision & risk map
* Domain-specific error taxonomy
* Acceptance & refusal criteria
* SME-authored evaluation question set

This alone is worth money.

---

## Why this workshop is a sales accelerator

From a consultantâ€™s perspective:

* it filters unserious clients
* it demonstrates rigor immediately
* it reframes GenAI as a governance problem
* it positions you as a facilitator of judgment, not a tool builder

---
# Grounding References
**Selective external grounding for evaluation-first claims**

This document maps **core claims** of the evaluation-first offering to **external practitioner and institutional sources**.  
References are used only where a senior stakeholder would reasonably challenge a claim as opinion.

The goal is credibility, not exhaustiveness.

---

## Claim 1  
### Most GenAI initiatives fail to translate from pilots to measurable business impact

**Why grounding is needed**  
Senior stakeholders will challenge â€œmost failâ€ unless it is tied to credible institutions.

**Primary sources (institutional)**
- MIT NANDA / Project NANDA report (â€œThe GenAI Divide: State of AI in Business 2025â€) reporting that *95% of organizations are getting zero return* and highlighting that outcomes are driven by approach rather than model quality.  
- Secondary coverage (use only when the PDF is too dense for stakeholders): Fortune / Tomâ€™s Hardware summaries referencing the report.

**Usage guidance**
- Phrase as â€œrecent MIT NANDA / Project NANDA reporting suggestsâ€¦â€ rather than â€œMIT provedâ€.
- Do not over-index on the number. Use it to open the door, then pivot to *why* (evaluation + integration + governance).

**Where to integrate**
- `03_sales_enablement/why_95_percent_fail_onepager.md` (primary)
- Optional: appendix note in `02_methodology/evaluation_first_principles.md`

---

## Claim 2  
### â€œModel qualityâ€ is not the bottleneck â€” evaluation and real-world constraints are

**Why grounding is needed**  
This is your differentiator: â€œevaluation-firstâ€ must not sound like a personal philosophy.

**Primary sources (big-company + platform)**
- OpenAI docs: evaluation best practices and guidance that generative AI is variable and traditional testing is insufficient; evals are needed to test systems in production-like conditions.  
- Anthropic research: â€œA statistical approach to model evaluationsâ€ stresses evaluation rigor, measurement uncertainty, and experimental design principles.

**Usage guidance**
- Use OpenAI/Anthropic as *credibility anchors* for â€œevals are required,â€ not as â€œendorsementsâ€ of your service.
- Keep this section very short in client-facing docs.

**Where to integrate**
- `03_sales_enablement/why_95_percent_fail_onepager.md`
- `02_methodology/evaluation_first_principles.md` (core grounding)

---

## Claim 3  
### Evaluation must be domain-specific and SME-defined

**Why grounding is needed**  
Tool-vendors and engineers often claim generic benchmarks and automated evals are enough.

**Primary sources (Ivy-league / domain eval)**
- Stanford HAI (medical/health contexts): emphasizes holistic, real-world, expert-grounded evaluation approaches for LLMs in specialized domains.

**Usage guidance**
- Emphasize SMEs define **acceptability + failure modes**, not model internals.
- Phrase as â€œdomain experts anchor evaluation to real task demands.â€

**Where to integrate**
- `02_methodology/evaluation_first_principles.md`
- Brief rationale line in `02_methodology/sme_workshop_agenda.md`

---

## Claim 4  
### Refusal / abstention is a required safety & governance control (not a failure)

**Why grounding is needed (light)**  
Some stakeholders believe partial answers are better than refusals.

**Primary sources**
- OpenAI guidance around evals + system behavior (use lightly; frame refusal as part of â€œspecifying behavior before deployment,â€ not as moral philosophy).

**Usage guidance**
- Keep this to one paragraph + one reference.
- Treat refusal as a risk control: â€œmust refuse when criteria arenâ€™t met.â€

**Where to integrate**
- `02_methodology/failure_taxonomy_reference.md`

---

## Claim 5  
### Evaluation artifacts retain value even if the system is not deployed

**Grounding status**
âŒ No external grounding required.

**Rationale**
Governance and audit stakeholders already understand why decision frameworks and evaluation artifacts remain reusable.

---

## Claim 6  
### Evaluation-first reduces long-term risk and rework

**Grounding status**
âš ï¸ Avoid quantitative ROI claims until you have your own case studies.

**Rationale**
Without your own evidence, external ROI claims are easy to attack and create false expectations.

**Where to integrate**
Verbal only (for now). Optionally in future, after 1â€“2 engagements with permissioned case studies.

---

## Approved reference shortlist (do not expand casually)

**Institutional**
[1] MIT / Project NANDA â€” The GenAI Divide: State of AI in Business 2025
https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf
[5] Stanford Human-Centered AI â€” Evaluation in High-Stakes Domains
https://hai.stanford.edu/research

**Big-company / platform**
References
[2] Hamel Husain â€” Evals Before Optimization
https://hamel.dev/blog/posts/evals-faq/
[3] Eugene Yan â€” Productizing Machine Learning / ML System Failure Modes
https://eugeneyan.com/writing/
[4] Anthropic â€” A Statistical Approach to Model Evaluations
https://www.anthropic.com/research/statistical-approach-to-model-evals
[6] Jason Snyder (Forbes) â€” MIT Finds 95% of GenAI Pilots Fail Because Companies Avoid Friction
https://www.forbes.com/sites/jasonsnyder/2025/08/26/mit-finds-95-of-genai-pilots-fail-because-companies-avoid-friction/
[7] Shreya Shankar â€” AI Engineering: The Missing Discipline
https://www.sh-reya.com/blog/ai-engineering-short/


---

# The Evaluation-First Mental Model

## How trustworthy AI-assisted knowledge systems are decided, not assumed

---

## Core idea (one sentence)

> **Trustworthy AI systems are not built first and evaluated later â€” they are evaluated first, and only then allowed to exist.**

---

## The model at a glance (conceptual flow)

```
Domain & Risk Context
        â†“
SME-Defined Acceptance Criteria
        â†“
Failure Taxonomy
        â†“
Evaluation & Evidence
        â†“
Decision: Proceed Â· Iterate Â· Stop
```

Each layer constrains the next.
Skipping a layer invalidates everything downstream.

---

## Layer 1 â€” Domain & risk context

**Key question**

> What decision would this system support, and what is the cost of being wrong?

**What is defined**

* decision scope
* accountability
* regulatory or governance constraints
* unacceptable outcomes

**Why this matters**
Without explicit risk context, â€œaccuracyâ€ is meaningless.

---

## Layer 2 â€” SME-defined acceptance criteria

**Key question**

> What does â€œacceptable behaviorâ€ mean in this domain?

**What is defined**

* what a correct answer looks like
* what errors are tolerable vs unacceptable
* when the system must refuse to answer

**Who defines this**

* Subject Matter Experts (not engineers, not vendors)

**Why this matters**
Only domain experts can define decision acceptability.

---

## Layer 3 â€” Failure taxonomy

**Key question**

> How can this system fail in ways that matter?

**What is defined**

* hallucination
* unsupported inference
* omission
* overconfidence
* refusal failure
* misalignment with domain norms

**Why this matters**
Trust is determined by *how systems fail*, not how often they succeed.

---

## Layer 4 â€” Evaluation & evidence

**Key question**

> How does the system behave when tested against real decision scenarios?

**What happens**

* evaluation datasets reflect real use cases
* system outputs are inspected and annotated
* failures are classified using the taxonomy
* trade-offs and irreducible risks are surfaced

**What this produces**

* evidence, not impressions
* traceable evaluation artifacts

---

## Layer 5 â€” Decision

**Key question**

> Based on evidence, should this system exist?

**Possible outcomes**

* **Proceed** â€” risks are acceptable
* **Iterate** â€” risks can be reduced
* **Stop** â€” risks are unacceptable

All three outcomes are valid.

**Why this matters**
The goal is not deployment.
The goal is a **defensible decision**.

---

## What this model prevents

This model explicitly prevents:

* demo-driven deployments
* optimism bias (â€œit will get betterâ€)
* model-centric decision-making
* late discovery of unacceptable behavior
* post-hoc governance justification

---

## What this model enables

This model enables:

* early detection of no-go scenarios
* transparent trade-off discussions
* SME authority without technical burden
* audit-ready decision trails
* trust that survives scrutiny

---

## Relationship to tooling

This model is **tool-agnostic**.

Evaluation tooling (e.g. Evaluizer) is used to operationalize:

* annotation
* aggregation
* regression tracking

Tooling does not define:

* acceptance criteria
* failure categories
* decisions

Those remain human and domain-anchored.

---

## One-line takeaway (for slides or website)

> **Evaluation defines whether an AI system is allowed to exist â€” not how impressive it looks.**

---
# Evaluation-First GenAI Consulting â€” Knowledge Base

This repository contains the internal knowledge base, methodologies, and client-facing artifacts for an **evaluation-first consulting practice** focused on AI-assisted knowledge systems.

The core principle is simple:

> If an AI system cannot be meaningfully evaluated in its domain context, it should not be deployed.

The materials in this repository support structured engagements that help organizations decide **whether**, **how**, and **under which constraints** GenAI systems should be used.

---

## What this repository is

- A **structured consulting playbook**
- A collection of **reusable, rigorously defined artifacts**
- A separation of:
  - strategy
  - methodology
  - sales enablement
  - delivery
  - governance

It is designed to support:
- regulated and public-sector environments
- evaluation-first GenAI system design
- SME-driven acceptance criteria
- transparent risk and failure analysis

---

## What this repository is not

- A software product
- A production system
- A marketing website
- A collection of ad-hoc notes

All materials are designed to be **inspectable, explainable, and reusable**.

---

## Repository structure

```text
ai-eval-consulting/
â”‚
â”œâ”€â”€ 00_strategy/
â”‚   â”œâ”€â”€ positioning.md
â”‚   â”œâ”€â”€ product_overview.md
â”‚   â””â”€â”€ differentiation_evaluation_first.md
â”‚
â”œâ”€â”€ 01_services/
â”‚   â”œâ”€â”€ services_page.md
â”‚   â”œâ”€â”€ engagement_model.md
â”‚   â””â”€â”€ scope_out_of_scope.md
â”‚
â”œâ”€â”€ 02_methodology/
â”‚   â”œâ”€â”€ evaluation_first_principles.md
â”‚   â”œâ”€â”€ sme_workshop_agenda.md
â”‚   â”œâ”€â”€ evaluation_design_framework.md
â”‚   â””â”€â”€ failure_taxonomy_reference.md
â”‚
â”œâ”€â”€ 03_sales_enablement/
â”‚   â”œâ”€â”€ intro_call_script_15min.md
â”‚   â”œâ”€â”€ proposal_template_1pager.md
â”‚   â”œâ”€â”€ why_95_percent_fail_onepager.md
â”‚   â””â”€â”€ faq_objections.md
â”‚
â”œâ”€â”€ 04_delivery/
â”‚   â”œâ”€â”€ delivery_timeline_6_8_weeks.md
â”‚   â”œâ”€â”€ client_intake_checklist.md
â”‚   â””â”€â”€ decision_memo_template.md
â”‚
â”œâ”€â”€ 05_tools_evaluizer/
â”‚   â”œâ”€â”€ evaluizer_overview.md
â”‚   â”œâ”€â”€ evaluizer_demo_flow.md
â”‚   â”œâ”€â”€ evaluizer_client_handover.md
â”‚   â””â”€â”€ evaluizer_customization_notes.md
â”‚
â”œâ”€â”€ 06_governance_ip_coi/
â”‚   â”œâ”€â”€ conflict_of_interest_policy_internal.md
â”‚   â”œâ”€â”€ ip_boundary_statement_external.md
â”‚   â””â”€â”€ engagement_documentation_template.md
â”‚
â”œâ”€â”€ 07_case_materials/
â”‚   â”œâ”€â”€ ngo_case_study_draft.md
â”‚   â”œâ”€â”€ anonymized_examples.md
â”‚   â””â”€â”€ future_case_studies/
â”‚
â”œâ”€â”€ 08_website/
â”‚   â”œâ”€â”€ homepage_copy.md
â”‚   â”œâ”€â”€ services_page_copy.md
â”‚   â”œâ”€â”€ about_page_copy.md
â”‚   â””â”€â”€ contact_page_copy.md
â”‚
â””â”€â”€ 09_admin_legal/
    â”œâ”€â”€ contract_outline.md
    â”œâ”€â”€ pricing_notes.md
    â””â”€â”€ business_setup_notes.md

---

# Evaluizer Demo Flow

**Making evaluation concrete, inspectable, and maintainable**

## Purpose of the demo (internal)

* Reduce fear around â€œlong, unstructured evaluationâ€
* Show that evaluation is **operational**, not theoretical
* Demonstrate continuity **after** the engagement ends
* Reinforce that this is *not* model magic, but disciplined process

This demo is **not technical**.
It is about *how decisions are supported*.

---

## When to use this demo

* After a positive intro call
* After the SME Evaluation Workshop
* Before finalizing scope or contract
* With stakeholders worried about:

  * maintenance
  * regression
  * â€œwhat happens when you leaveâ€

---

## Demo duration

* **10â€“15 minutes**
* Live screen share *or* narrated screenshots
* No deep dives into code or models

---

## Demo structure (step-by-step)

---

## Step 1 â€” Start with the evaluation dataset (2â€“3 minutes)

**What you show**

The Evaluizer interface with:

* a list of evaluation questions
* grouped by risk tier (e.g. high-risk, informational, refusal cases)

**What you say**

> Before we look at any answers, we start with the evaluation set.
>
> These are questions defined or validated by your SMEs â€” not generic prompts.
>
> This dataset represents *what matters* in your domain, including edge cases and questions the system should refuse to answer.

**Key message**

* Evaluation starts with **domain judgment**, not model output.

---

## Step 2 â€” Inspect answers side-by-side (3 minutes)

**What you show**

* One evaluation question
* Multiple model outputs (or multiple prompt versions)
* Source references (if available)

**What you say**

> Here we can inspect answers side-by-side against the same question.
>
> The goal is not to ask â€œwhich answer sounds bestâ€, but:
>
> * Is it complete?
> * Is it grounded in the right documents?
> * Does it introduce information that should not be there?
> * Should it have refused instead?

**Key message**

* Answers are **inspectable**, not opaque.

---

## Step 3 â€” Manual annotation with SMEs (2â€“3 minutes)

**What you show**

* Annotation fields (pass / fail, error type, comments)

**What you say**

> SMEs can annotate answers directly:
>
> * mark failure modes (e.g. hallucination, omission)
> * flag unacceptable behavior
> * add short comments explaining why
>
> This turns expert judgment into a reusable evaluation artifact.

**Key message**

* SME knowledge is **captured**, not lost in meetings.

---

## Step 4 â€” Automated evaluation & scoring (2â€“3 minutes)

**What you show**

* Aggregate scores
* Pass/fail counts
* Error distribution by type

**What you say**

> Once acceptance criteria are defined, Evaluizer can run automated checks:
>
> * deterministic rules
> * LLM-based judges
> * or combinations of both
>
> These scores donâ€™t replace human judgment â€” they make changes *visible* and *comparable* over time.

**Key message**

* Evaluation is **repeatable**, not subjective every time.

---

## Step 5 â€” Comparing changes over time (2â€“3 minutes)

**What you show**

* Comparison between:

  * two prompt versions
  * or before/after a data change

**What you say**

> This is where evaluation really pays off.
>
> When something changes â€” a prompt, a document set, a model â€” you can immediately see:
>
> * what improved
> * what regressed
> * whether risk increased or decreased
>
> This prevents silent degradation.

**Key message**

* No more â€œit seemed better yesterdayâ€.

---

## Step 6 â€” Handover & continuity (2 minutes)

**What you show**

* Clean project structure
* Saved evaluation sets
* Clear naming/versioning

**What you say**

> At the end of the engagement, this setup is handed over:
>
> * your evaluation dataset
> * your acceptance criteria
> * your evaluation workflow
>
> Internal teams can continue running evaluations without my involvement.
>
> This avoids vendor lock-in and reduces long-term risk.

**Key message**

* You are **not dependent** on the consultant.

---

## What you explicitly do NOT demo

* Model internals
* Embeddings
* Vector databases
* Prompt â€œclevernessâ€
* Optimization tricks

If asked, you say:

> Those choices only matter *after* evaluation criteria are clear.

---

## Typical stakeholder reactions (and how to respond)

### â€œThis looks slower than just building somethingâ€

> Itâ€™s slower than a demo â€” and much faster than recovering trust after a failed deployment.

### â€œCan our team use this later?â€

> Yes. The evaluation setup is explicitly designed for internal continuation.

### â€œDoes this guarantee correctness?â€

> No system can guarantee correctness. This makes risks explicit and measurable.

---

## Positioning sentence (use verbatim)

> Evaluizer turns evaluation from a vague promise into a concrete, inspectable workflow that aligns AI behavior with domain risk.

---

## Consultantâ€™s internal note

This demo:

* reassures risk-averse stakeholders
* filters out â€œmove fastâ€ clients
* reinforces your evaluation-first positioning
* justifies your fee without talking about price

---

---

# 15-Minute Intro Call Script

## Evaluation-First Knowledge System Assessment & Prototype

**FINAL â€“ Canonical Version**

---

## Objective of the call (internal)

This call has **one purpose**:

> Determine whether an **evaluation-first assessment** is appropriate â€” and if so, whether to proceed to an SME Evaluation Workshop.

This is **not** a sales pitch.
This is a **fit check**.

---

## Tone & posture

* Calm
* Structured
* Conservative
* No hype
* No promises

If you sound excited, youâ€™re doing it wrong.

---

## 0:00â€“1:00 â€” Opening & framing

**You say (verbatim):**

> Thanks for taking the time.
>
> To keep this useful, I suggest a simple structure:
> Iâ€™ll briefly explain how I work (about two minutes), then Iâ€™ll ask a few questions about your context, and weâ€™ll end with a clear next step.
>
> Does that work for you?

If they say no â†’ already a warning sign.

---

## 1:00â€“3:00 â€” What you do (locked wording)

**You say (verbatim):**

> I work on **evaluation-first AI-assisted knowledge systems**.
>
> The focus is not on building chatbots or pilots, but on defining **acceptance criteria, failure modes, and decision risk** before any system is deployed.
>
> The outcome of my work is a clear recommendation to **proceed, iterate, or stop**, based on evidence â€” not a production system.

Stop here.
Do **not** elaborate unless asked.

---

## 3:00â€“11:00 â€” Qualification questions (core of the call)

Ask **only what you need to decide**.
Do not solve anything.

---

### A. Decision & accountability (mandatory)

1. **Decision**

> What decision would this system support, and what changes if the answer is good?

2. **Accountability**

> Who would remain accountable for outcomes if such a system were used?

If they cannot answer these â†’ no engagement.

---

### B. Risk & failure tolerance (mandatory)

3. **Cost of being wrong**

> What happens if the system gives a confident but incorrect answer?

4. **Unacceptable failure**

> Are there answer types that would be unacceptable, even if they are rarely wrong?

5. **Refusal**

> Are there questions the system should never answer and must refuse?

If refusal is not acceptable â†’ strong warning sign.

---

### C. Data & constraints (mandatory)

6. **Document reality**

> What types of documents would the system rely on?

7. **Constraints**

> Are there constraints around where data can be processed or who can access it?

---

### D. Organizational readiness (mandatory)

8. **SMEs**

> Who are the 2â€“4 Subject Matter Experts who could define acceptance criteria and failure modes?

9. **Decision owner**

> Who would make the final decision to proceed, iterate, or stop?

If SMEs or decision owner are missing â†’ do not proceed.

---

## 11:00â€“13:00 â€” Reflect & qualify out loud

**You say (paraphrase, but be crisp):**

> Let me reflect what I heard:
> Youâ€™re looking to support **[decision]**, the main risk is **[risk]**, the data is **[type]**, and governance constraints include **[constraint]**.
>
> Based on that, this **does / does not** sound like a good fit for an evaluation-first assessment.

Say *does not* if needed.
This is where trust is built.

---

## 13:00â€“15:00 â€” Close with one of three outcomes

---

### Outcome A â€” Proceed to SME Evaluation Workshop (preferred)

**You say:**

> The appropriate next step would be a **structured SME Evaluation Workshop**.
>
> It takes 90â€“120 minutes and results in domain-specific acceptance criteria and an initial evaluation dataset.
>
> SME involvement is typically **2â€“4 hours total**.
>
> If that sounds reasonable, I can outline the workshop and confirm scope.

---

### Outcome B â€” Pause (insufficient readiness)

**You say:**

> At the moment, the missing piece is **[SMEs / decision owner / data access]**.
>
> Without that, any work would likely turn into a demo without evaluation rigor.
>
> If that changes, Iâ€™d be happy to revisit.

This saves you time and reputation.

---

### Outcome C â€” Decline (hard no)

**You say:**

> Based on what you described, this engagement would not be appropriate.
>
> My work is intentionally conservative and not designed for rapid experimentation or deployment.
>
> I donâ€™t want to create false expectations.

This will *increase* respect, not reduce it.

---

## If asked about tooling (Evaluizer) â€” 20 seconds max

**You say (verbatim):**

> An evaluation interface is used during the engagement and handed over so internal teams can continue evaluation independently.
>
> Tooling is secondary â€” evaluation criteria come first.

Then stop.

---

## What you must NOT say (ever)

* No accuracy numbers
* No â€œwe reduce hallucinationsâ€
* No â€œenterprise-gradeâ€ buzzwords
* No model or architecture talk
* No promises about outcomes

If asked, redirect to evaluation.

---

## Red flags (end early if â‰¥2)

* â€œWe just want something quickâ€
* No SME availability
* No decision owner
* Resistance to refusal design
* Expectation of production deployment
* Tool-first mindset

---

## Success criteria for this call (internal)

* Decision owner identified
* SMEs identified
* Risk tolerance understood
* Constraints explicit
* Clear next step chosen

If these are not met â†’ do not proceed.

---

---

# Proposal

## Evaluation-First Knowledge System Assessment & Prototype

**Client:** [Organization name]
**Prepared for:** [Decision owner / unit]
**Prepared by:** Fatih Ãœnal
**Date:** [Date]

---

## 1. Context & objective

[Organization] is exploring the use of an **AI-assisted knowledge system** to support decisions related to:

* **Decisions supported:** [short description]
* **Primary users:** [roles / functions]
* **Primary risk:** [e.g. incorrect, incomplete, or misleading answers]

The objective of this engagement is **not to deploy a production system**, but to determine **whether such a system can be trusted in this domain**, and under which constraints.

---

## 2. Why an evaluation-first approach

Most GenAI initiatives fail not because models are weak, but because **success and failure are never defined in a way that reflects real organizational risk**.

As a result:

* unacceptable errors are discovered late
* refusal and uncertainty are not designed
* systems perform well in demos but fail in practice

This engagement is based on a single principle:

> **If a system cannot be meaningfully evaluated in its domain context, it should not be deployed.**

---

## 3. Scope of the engagement (6â€“8 weeks)

The engagement is a **fixed-scope, time-boxed assessment** structured around evaluation before implementation.

---

### 3.1 Use-case & risk framing

We clarify:

* which decisions the system would support
* who remains accountable for outcomes
* what constitutes unacceptable vs tolerable failure
* where the system must refuse to answer

**Deliverable**
Problem & risk framing document

---

### 3.2 Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* audit and governance requirements
* data gaps and no-go signals

A recommendation **not to proceed** is considered a valid outcome.

**Deliverable**
Data readiness & governance memo

---

### 3.3 Joint evaluation & system design (with SMEs)

In structured working sessions, **Subject Matter Experts co-define**:

* domain-specific acceptance criteria
* failure modes and refusal rules
* evaluation datasets reflecting real decision risk

Evaluation design precedes system optimization.

**Deliverables**

* Evaluation design document
* SME-validated evaluation dataset

---

### 3.4 Evaluation-guided prototype

A limited-scope prototype is implemented as an **instrumented test harness**, not a production system.

It is used to:

* run evaluations against agreed criteria
* inspect failure modes and trade-offs
* identify irreducible risks

**Deliverables**

* Instrumented prototype
* Evaluation report with failure analysis

---

### 3.5 Decision memo

The engagement concludes with a structured recommendation to:

> **Proceed Â· Iterate Â· Stop**

**Deliverable**
Decision memo suitable for management and governance review

---

## 4. Engagement success criteria

This engagement is successful if, at the end of the assessment, the organization has:

* a documented evaluation framework grounded in domain risk
* a prototype tested against SME-defined acceptance criteria
* a clear, defensible decision to proceed, iterate, or stop

No production deployment is required for success.

---

## 5. Evaluation infrastructure & continuity

An evaluation interface based on **Evaluizer** is used during the engagement and handed over so internal teams can continue evaluation independently.

This enables:

* transparent inspection of system behavior
* repeatable evaluation over time
* regression detection as data or prompts change

No vendor lock-in is introduced.

---

## 6. Roles & time commitment

**Client provides**

* access to representative documents
* availability of **2â€“4 Subject Matter Experts**
* one decision owner for scoping and sign-off

**Time commitment**

* SME involvement is typically limited to **2â€“4 hours total**, primarily through a structured evaluation workshop

**Consultant provides**

* evaluation-first methodology
* facilitation of SME workshops
* prototype and evaluation artifacts
* independent recommendation

---

## 7. Engagement model

* Fixed scope
* Fixed duration (6â€“8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

**This engagement is intentionally conservative and is not designed for rapid experimentation or production deployment.**

---

## 8. Independence & IP

All work is conducted independently and uses only client-provided or publicly available materials.
No confidential or proprietary information of third parties is reused or derived.

---

## 9. If the recommendation is to stop

If the recommendation is not to proceed, **all evaluation artifacts, decision frameworks, and findings remain usable** for future initiatives, audits, or internal reviews.

Stopping is treated as a valid and valuable outcome.

---

## 10. Commercials

**Fee:** CHF [XXâ€™000] (fixed)
**Payment terms:** [e.g. 50% start / 50% end]
**Start date:** [date]

---

## 11. Next step

Upon confirmation, the engagement starts with scheduling the **SME Evaluation Workshop**, followed by document review and detailed planning.

---

---

# Why 95% of GenAI Initiatives Fail

## And What the 5% That Work Do Differently

---

## Executive summary

Most Generative AI initiatives do not fail because the models are weak.
They fail because **organizations deploy systems without defining what acceptable behavior means in their specific domain**.

Recent institutional reporting suggests that a large majority of organizations experimenting with GenAI see **little to no measurable business value**, despite access to increasingly capable models [1]. The decisive factor is not model choice, but whether **evaluation, risk, and governance are addressed before deployment**.

---

## The real failure mode (not the one people talk about)

The dominant failure pattern across GenAI initiatives is **not**:

* lack of data
* poor prompts
* insufficient model capability

Instead, systems fail because:

* success and failure are never explicitly defined
* unacceptable errors are discovered only after deployment
* refusal and uncertainty are not designed as first-class behaviors
* evaluation is treated as an afterthought rather than infrastructure

Practitioners working on real-world ML and GenAI systems consistently report that **optimization without evaluation is meaningless** â€” systems are tuned without knowing whether their behavior is acceptable in real decision contexts [2][3].

---

## Why better models donâ€™t fix this

Modern foundation models are already capable of producing fluent and confident answers across many domains. The problem is that **fluency is not correctness**, and correctness is not the same as *acceptability*.

Research from leading AI labs emphasizes that model behavior is inherently variable and context-dependent, and that meaningful progress depends on **statistically sound, task-specific evaluation**, not generic benchmarks [4].

In other words:

> Model quality is no longer the bottleneck.
> **Evaluation quality is.**

---

## The missing ingredient: domain-specific evaluation

What separates the minority of GenAI systems that work in practice is not architecture, but process.

Effective systems are built around:

* **domain-specific acceptance criteria**, not generic accuracy scores
* **Subject Matter Experts defining what is acceptable**, risky, or unacceptable
* **explicit failure modes**, including when the system must refuse to answer

Applied AI research in regulated and high-stakes domains shows that **expert-grounded, task-specific evaluation** is necessary to prevent silent failure and misplaced trust [5].

---

## Why â€œfrictionâ€ is unavoidable (and necessary)

Some organizations interpret evaluation, governance, and SME involvement as â€œfrictionâ€ that slows innovation. Empirical reporting suggests the opposite: **avoiding this friction is a key reason pilots fail to translate into durable value** [1][6].

The organizations that succeed accept early friction in exchange for:

* predictable system behavior
* defensible deployment decisions
* long-term trust and adoption

Avoiding evaluation does not remove friction â€” it **defers it**, often until failure is public and costly.

---

## The AI engineering reality

Modern AI engineering practice increasingly recognizes that model-centric thinking is insufficient. What matters is **end-to-end system behavior**, including evaluation loops, human feedback, and acceptance criteria.

A concise overview of this shift is illustrated in Shreya Shankarâ€™s AI engineering framework, which highlights evaluation and feedback as central system components rather than post-hoc checks [7].

---

## What this means in practice

A GenAI initiative should not begin with:

* model selection
* prompt tuning
* tool integration

It should begin by answering:

* What decisions will this system support?
* What errors are unacceptable?
* When must the system refuse to answer?
* How will we know if the system is safe to use?

If these questions cannot be answered, deployment is premature.

---

## The evaluation-first alternative

An **evaluation-first approach** inverts the usual process:

1. Define domain-specific acceptance criteria with SMEs
2. Design evaluation datasets reflecting real decision risk
3. Test system behavior against those criteria
4. Decide to **proceed, iterate, or stop** based on evidence

Only systems that survive this process should move toward deployment.

---

## Closing thought

The gap between GenAI demos and GenAI systems that deliver real value is not intelligence â€” it is **discipline**.

Evaluation is not overhead.
It is the mechanism that turns powerful models into trustworthy systems.

---

## References

[1] MIT / Project NANDA â€” *The GenAI Divide: State of AI in Business 2025*
[https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf](https://mlq.ai/media/quarterly_decks/v0.1_State_of_AI_in_Business_2025_Report.pdf)

[2] Hamel Husain â€” *Evals Before Optimization*
[https://hamel.dev/blog/posts/evals-faq/](https://hamel.dev/blog/posts/evals-faq/)

[3] Eugene Yan â€” *Productizing Machine Learning* / *ML System Failure Modes*
[https://eugeneyan.com/writing/](https://eugeneyan.com/writing/)

[4] Anthropic â€” *A Statistical Approach to Model Evaluations*
[https://www.anthropic.com/research/statistical-approach-to-model-evals](https://www.anthropic.com/research/statistical-approach-to-model-evals)

[5] Stanford Human-Centered AI â€” Evaluation in High-Stakes Domains
[https://hai.stanford.edu/research](https://hai.stanford.edu/research)

[6] Jason Snyder (Forbes) â€” *MIT Finds 95% of GenAI Pilots Fail Because Companies Avoid Friction*
[https://www.forbes.com/sites/jasonsnyder/2025/08/26/mit-finds-95-of-genai-pilots-fail-because-companies-avoid-friction/](https://www.forbes.com/sites/jasonsnyder/2025/08/26/mit-finds-95-of-genai-pilots-fail-because-companies-avoid-friction/)

[7] Shreya Shankar â€” *AI Engineering: The Missing Discipline*
[https://www.sh-reya.com/blog/ai-engineering-short/](https://www.sh-reya.com/blog/ai-engineering-short/)

---
