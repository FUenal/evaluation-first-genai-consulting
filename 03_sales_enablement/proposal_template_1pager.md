
---

# Proposal

## Evaluation-First Knowledge System Assessment & Prototype

**Client:** [Organization name]
**Prepared for:** [Decision owner / unit]
**Prepared by:** Fatih Ünal
**Date:** [Date]

---

## 1. Context & objective

[Organization] is exploring the use of an **AI-assisted knowledge system** to support decisions related to:

* **Decisions supported:** [short description]
* **Primary users:** [roles / functions]
* **Primary risk:** [e.g. incorrect, incomplete, or misleading answers]

The objective of this engagement is **not to deploy a production system**, but to determine **whether such a system can be trusted in this domain**, and under which constraints.

---

## 2. Why an evaluation-first approach

Most GenAI initiatives fail not because models are weak, but because **success and failure are never defined in a way that reflects real organizational risk**.

As a result:

* unacceptable errors are discovered late
* refusal and uncertainty are not designed
* systems perform well in demos but fail in practice

This engagement is based on a single principle:

> **If a system cannot be meaningfully evaluated in its domain context, it should not be deployed.**

---

## 3. Scope of the engagement (6–8 weeks)

The engagement is a **fixed-scope, time-boxed assessment** structured around evaluation before implementation.

---

### 3.1 Use-case & risk framing

We clarify:

* which decisions the system would support
* who remains accountable for outcomes
* what constitutes unacceptable vs tolerable failure
* where the system must refuse to answer

**Deliverable**
Problem & risk framing document

---

### 3.2 Data & governance readiness

We assess:

* document types and structure
* sensitivity and access constraints
* audit and governance requirements
* data gaps and no-go signals

A recommendation **not to proceed** is considered a valid outcome.

**Deliverable**
Data readiness & governance memo

---

### 3.3 Joint evaluation & system design (with SMEs)

In structured working sessions, **Subject Matter Experts co-define**:

* domain-specific acceptance criteria
* failure modes and refusal rules
* evaluation datasets reflecting real decision risk

Evaluation design precedes system optimization.

**Deliverables**

* Evaluation design document
* SME-validated evaluation dataset

---

### 3.4 Evaluation-guided prototype

A limited-scope prototype is implemented as an **instrumented test harness**, not a production system.

It is used to:

* run evaluations against agreed criteria
* inspect failure modes and trade-offs
* identify irreducible risks

**Deliverables**

* Instrumented prototype
* Evaluation report with failure analysis

---

### 3.5 Decision memo

The engagement concludes with a structured recommendation to:

> **Proceed · Iterate · Stop**

**Deliverable**
Decision memo suitable for management and governance review

---

## 4. Engagement success criteria

This engagement is successful if, at the end of the assessment, the organization has:

* a documented evaluation framework grounded in domain risk
* a prototype tested against SME-defined acceptance criteria
* a clear, defensible decision to proceed, iterate, or stop

No production deployment is required for success.

---

## 5. Evaluation infrastructure & continuity

An evaluation interface based on **Evaluizer** is used during the engagement and handed over so internal teams can continue evaluation independently.

This enables:

* transparent inspection of system behavior
* repeatable evaluation over time
* regression detection as data or prompts change

No vendor lock-in is introduced.

---

## 6. Roles & time commitment

**Client provides**

* access to representative documents
* availability of **2–4 Subject Matter Experts**
* one decision owner for scoping and sign-off

**Time commitment**

* SME involvement is typically limited to **2–4 hours total**, primarily through a structured evaluation workshop

**Consultant provides**

* evaluation-first methodology
* facilitation of SME workshops
* prototype and evaluation artifacts
* independent recommendation

---

## 7. Engagement model

* Fixed scope
* Fixed duration (6–8 weeks)
* Limited number of concurrent clients
* No production deployment
* No ongoing operational responsibility

**This engagement is intentionally conservative and is not designed for rapid experimentation or production deployment.**

---

## 8. Independence & IP

All work is conducted independently and uses only client-provided or publicly available materials.
No confidential or proprietary information of third parties is reused or derived.

---

## 9. If the recommendation is to stop

If the recommendation is not to proceed, **all evaluation artifacts, decision frameworks, and findings remain usable** for future initiatives, audits, or internal reviews.

Stopping is treated as a valid and valuable outcome.

---

## 10. Commercials

**Fee:** CHF [XX’000] (fixed)
**Payment terms:** [e.g. 50% start / 50% end]
**Start date:** [date]

---

## 11. Next step

Upon confirmation, the engagement starts with scheduling the **SME Evaluation Workshop**, followed by document review and detailed planning.

---
