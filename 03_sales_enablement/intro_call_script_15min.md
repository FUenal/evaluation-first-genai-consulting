
---

# 15-Minute Intro Call Script

## Evaluation-First Knowledge System Assessment & Prototype

**FINAL – Canonical Version**

---

## Objective of the call (internal)

This call has **one purpose**:

> Determine whether an **evaluation-first assessment** is appropriate — and if so, whether to proceed to an SME Evaluation Workshop.

This is **not** a sales pitch.
This is a **fit check**.

---

## Tone & posture

* Calm
* Structured
* Conservative
* No hype
* No promises

If you sound excited, you’re doing it wrong.

---

## 0:00–1:00 — Opening & framing

**You say (verbatim):**

> Thanks for taking the time.
>
> To keep this useful, I suggest a simple structure:
> I’ll briefly explain how I work (about two minutes), then I’ll ask a few questions about your context, and we’ll end with a clear next step.
>
> Does that work for you?

If they say no → already a warning sign.

---

## 1:00–3:00 — What you do (locked wording)

**You say (verbatim):**

> I work on **evaluation-first AI-assisted knowledge systems**.
>
> The focus is not on building chatbots or pilots, but on defining **acceptance criteria, failure modes, and decision risk** before any system is deployed.
>
> The outcome of my work is a clear recommendation to **proceed, iterate, or stop**, based on evidence — not a production system.

Stop here.
Do **not** elaborate unless asked.

---

## 3:00–11:00 — Qualification questions (core of the call)

Ask **only what you need to decide**.
Do not solve anything.

---

### A. Decision & accountability (mandatory)

1. **Decision**

> What decision would this system support, and what changes if the answer is good?

2. **Accountability**

> Who would remain accountable for outcomes if such a system were used?

If they cannot answer these → no engagement.

---

### B. Risk & failure tolerance (mandatory)

3. **Cost of being wrong**

> What happens if the system gives a confident but incorrect answer?

4. **Unacceptable failure**

> Are there answer types that would be unacceptable, even if they are rarely wrong?

5. **Refusal**

> Are there questions the system should never answer and must refuse?

If refusal is not acceptable → strong warning sign.

---

### C. Data & constraints (mandatory)

6. **Document reality**

> What types of documents would the system rely on?

7. **Constraints**

> Are there constraints around where data can be processed or who can access it?

---

### D. Organizational readiness (mandatory)

8. **SMEs**

> Who are the 2–4 Subject Matter Experts who could define acceptance criteria and failure modes?

9. **Decision owner**

> Who would make the final decision to proceed, iterate, or stop?

If SMEs or decision owner are missing → do not proceed.

---

## 11:00–13:00 — Reflect & qualify out loud

**You say (paraphrase, but be crisp):**

> Let me reflect what I heard:
> You’re looking to support **[decision]**, the main risk is **[risk]**, the data is **[type]**, and governance constraints include **[constraint]**.
>
> Based on that, this **does / does not** sound like a good fit for an evaluation-first assessment.

Say *does not* if needed.
This is where trust is built.

---

## 13:00–15:00 — Close with one of three outcomes

---

### Outcome A — Proceed to SME Evaluation Workshop (preferred)

**You say:**

> The appropriate next step would be a **structured SME Evaluation Workshop**.
>
> It takes 90–120 minutes and results in domain-specific acceptance criteria and an initial evaluation dataset.
>
> SME involvement is typically **2–4 hours total**.
>
> If that sounds reasonable, I can outline the workshop and confirm scope.

---

### Outcome B — Pause (insufficient readiness)

**You say:**

> At the moment, the missing piece is **[SMEs / decision owner / data access]**.
>
> Without that, any work would likely turn into a demo without evaluation rigor.
>
> If that changes, I’d be happy to revisit.

This saves you time and reputation.

---

### Outcome C — Decline (hard no)

**You say:**

> Based on what you described, this engagement would not be appropriate.
>
> My work is intentionally conservative and not designed for rapid experimentation or deployment.
>
> I don’t want to create false expectations.

This will *increase* respect, not reduce it.

---

## If asked about tooling (Evaluizer) — 20 seconds max

**You say (verbatim):**

> An evaluation interface is used during the engagement and handed over so internal teams can continue evaluation independently.
>
> Tooling is secondary — evaluation criteria come first.

Then stop.

---

## What you must NOT say (ever)

* No accuracy numbers
* No “we reduce hallucinations”
* No “enterprise-grade” buzzwords
* No model or architecture talk
* No promises about outcomes

If asked, redirect to evaluation.

---

## Red flags (end early if ≥2)

* “We just want something quick”
* No SME availability
* No decision owner
* Resistance to refusal design
* Expectation of production deployment
* Tool-first mindset

---

## Success criteria for this call (internal)

* Decision owner identified
* SMEs identified
* Risk tolerance understood
* Constraints explicit
* Clear next step chosen

If these are not met → do not proceed.

---
